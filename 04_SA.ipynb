{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uncertainty analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SA_all import SA\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "from plots import *\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 Combination: (0.5, 'mean', 'observed', 'FSeq', 'Gauss-Newton', 0.01, True, True, True)\n",
      "#################################################################################################################################################################### 03 DETERMINISTIC MODELLING ###################################################\n",
      "inv_columns Index(['EC_0.30', 'EC_0.60', 'EC_1.00', 'EC_2.00', 'EC_end'], dtype='object')\n",
      "current_results [{'Extract': 0.5, 'Samples location': 'mean', 'Interface': 'observed', 'Forward_Model': 'FSeq', 'Minimization_Method': 'Gauss-Newton', 'Alpha': 0.01, 'remove_coil': True, 'start_avg': True, 'constrain': True, 'Det': 'LT', 'R2': -7.812, 'RMSE': 0.256, 'MAE': 0.218, 'STD': 0.18, '0R2': -0.006, '0RMSE': 0.175, 'R2_INV': 0.9789587938613966}, {'Extract': 0.5, 'Samples location': 'mean', 'Interface': 'observed', 'Forward_Model': 'FSeq', 'Minimization_Method': 'Gauss-Newton', 'Alpha': 0.01, 'remove_coil': True, 'start_avg': True, 'constrain': True, 'Det': 'ID', 'R2': -9.528, 'RMSE': 0.286, 'MAE': 0.247, 'STD': 0.178, '0R2': 0.921, '0RMSE': 0.048, 'R2_INV': 0.9789587938613966}, {'Extract': 0.5, 'Samples location': 'mean', 'Interface': 'observed', 'Forward_Model': 'FSeq', 'Minimization_Method': 'Gauss-Newton', 'Alpha': 0.01, 'remove_coil': True, 'start_avg': True, 'constrain': True, 'Det': 'LS', 'R2': -7.786, 'RMSE': 0.255, 'MAE': 0.216, 'STD': 0.18, '0R2': 0.26, '0RMSE': 0.15, 'R2_INV': 0.9789587938613966}]\n",
      "Iteration: 1 Combination: (0.5, 'mean', 'observed', 'FSeq', 'Gauss-Newton', 0.01, True, True, False)\n",
      "#################################################################################################################################################################### 03 DETERMINISTIC MODELLING ###################################################\n",
      "inv_columns Index(['EC_0.30', 'EC_0.60', 'EC_1.00', 'EC_2.00', 'EC_end'], dtype='object')\n",
      "current_results [{'Extract': 0.5, 'Samples location': 'mean', 'Interface': 'observed', 'Forward_Model': 'FSeq', 'Minimization_Method': 'Gauss-Newton', 'Alpha': 0.01, 'remove_coil': True, 'start_avg': True, 'constrain': False, 'Det': 'LT', 'R2': -7.812, 'RMSE': 0.256, 'MAE': 0.218, 'STD': 0.18, '0R2': -0.006, '0RMSE': 0.175, 'R2_INV': 0.9789587938613966}, {'Extract': 0.5, 'Samples location': 'mean', 'Interface': 'observed', 'Forward_Model': 'FSeq', 'Minimization_Method': 'Gauss-Newton', 'Alpha': 0.01, 'remove_coil': True, 'start_avg': True, 'constrain': False, 'Det': 'ID', 'R2': -9.528, 'RMSE': 0.286, 'MAE': 0.247, 'STD': 0.178, '0R2': 0.921, '0RMSE': 0.048, 'R2_INV': 0.9789587938613966}, {'Extract': 0.5, 'Samples location': 'mean', 'Interface': 'observed', 'Forward_Model': 'FSeq', 'Minimization_Method': 'Gauss-Newton', 'Alpha': 0.01, 'remove_coil': True, 'start_avg': True, 'constrain': False, 'Det': 'LS', 'R2': -7.786, 'RMSE': 0.255, 'MAE': 0.216, 'STD': 0.18, '0R2': 0.26, '0RMSE': 0.15, 'R2_INV': 0.9789587938613966}]\n",
      "Iteration: 2 Combination: (0.5, 'mean', 'observed', 'FSeq', 'Gauss-Newton', 0.01, True, False, True)\n",
      "#################################################################################################################################################################### 03 DETERMINISTIC MODELLING ###################################################\n",
      "inv_columns Index(['EC_0.30', 'EC_0.60', 'EC_1.00', 'EC_2.00', 'EC_end'], dtype='object')\n",
      "current_results [{'Extract': 0.5, 'Samples location': 'mean', 'Interface': 'observed', 'Forward_Model': 'FSeq', 'Minimization_Method': 'Gauss-Newton', 'Alpha': 0.01, 'remove_coil': True, 'start_avg': False, 'constrain': True, 'Det': 'LT', 'R2': -7.788, 'RMSE': 0.256, 'MAE': 0.218, 'STD': 0.18, '0R2': -0.02, '0RMSE': 0.176, 'R2_INV': 0.9791564478931912}, {'Extract': 0.5, 'Samples location': 'mean', 'Interface': 'observed', 'Forward_Model': 'FSeq', 'Minimization_Method': 'Gauss-Newton', 'Alpha': 0.01, 'remove_coil': True, 'start_avg': False, 'constrain': True, 'Det': 'ID', 'R2': -9.52, 'RMSE': 0.286, 'MAE': 0.247, 'STD': 0.176, '0R2': 0.917, '0RMSE': 0.05, 'R2_INV': 0.9791564478931912}, {'Extract': 0.5, 'Samples location': 'mean', 'Interface': 'observed', 'Forward_Model': 'FSeq', 'Minimization_Method': 'Gauss-Newton', 'Alpha': 0.01, 'remove_coil': True, 'start_avg': False, 'constrain': True, 'Det': 'LS', 'R2': -7.81, 'RMSE': 0.255, 'MAE': 0.217, 'STD': 0.18, '0R2': 0.249, '0RMSE': 0.15, 'R2_INV': 0.9791564478931912}]\n",
      "Iteration: 3 Combination: (0.5, 'mean', 'observed', 'FSeq', 'Gauss-Newton', 0.01, True, False, False)\n",
      "#################################################################################################################################################################### 03 DETERMINISTIC MODELLING ###################################################\n",
      "inv_columns Index(['EC_0.30', 'EC_0.60', 'EC_1.00', 'EC_2.00', 'EC_end'], dtype='object')\n",
      "current_results [{'Extract': 0.5, 'Samples location': 'mean', 'Interface': 'observed', 'Forward_Model': 'FSeq', 'Minimization_Method': 'Gauss-Newton', 'Alpha': 0.01, 'remove_coil': True, 'start_avg': False, 'constrain': False, 'Det': 'LT', 'R2': -7.788, 'RMSE': 0.256, 'MAE': 0.218, 'STD': 0.18, '0R2': -0.02, '0RMSE': 0.176, 'R2_INV': 0.9791564478931912}, {'Extract': 0.5, 'Samples location': 'mean', 'Interface': 'observed', 'Forward_Model': 'FSeq', 'Minimization_Method': 'Gauss-Newton', 'Alpha': 0.01, 'remove_coil': True, 'start_avg': False, 'constrain': False, 'Det': 'ID', 'R2': -9.52, 'RMSE': 0.286, 'MAE': 0.247, 'STD': 0.176, '0R2': 0.917, '0RMSE': 0.05, 'R2_INV': 0.9791564478931912}, {'Extract': 0.5, 'Samples location': 'mean', 'Interface': 'observed', 'Forward_Model': 'FSeq', 'Minimization_Method': 'Gauss-Newton', 'Alpha': 0.01, 'remove_coil': True, 'start_avg': False, 'constrain': False, 'Det': 'LS', 'R2': -7.81, 'RMSE': 0.255, 'MAE': 0.217, 'STD': 0.18, '0R2': 0.249, '0RMSE': 0.15, 'R2_INV': 0.9791564478931912}]\n",
      "Iteration: 4 Combination: (0.5, 'mean', 'observed', 'FSeq', 'Gauss-Newton', 0.01, False, True, True)\n",
      "#################################################################################################################################################################### 03 DETERMINISTIC MODELLING ###################################################\n",
      "inv_columns Index(['EC_0.30', 'EC_0.60', 'EC_1.00', 'EC_2.00', 'EC_end'], dtype='object')\n",
      "current_results [{'Extract': 0.5, 'Samples location': 'mean', 'Interface': 'observed', 'Forward_Model': 'FSeq', 'Minimization_Method': 'Gauss-Newton', 'Alpha': 0.01, 'remove_coil': False, 'start_avg': True, 'constrain': True, 'Det': 'LT', 'R2': -7.308, 'RMSE': 0.247, 'MAE': 0.211, 'STD': 0.212, '0R2': 0.234, '0RMSE': 0.148, 'R2_INV': 0.921658370720488}, {'Extract': 0.5, 'Samples location': 'mean', 'Interface': 'observed', 'Forward_Model': 'FSeq', 'Minimization_Method': 'Gauss-Newton', 'Alpha': 0.01, 'remove_coil': False, 'start_avg': True, 'constrain': True, 'Det': 'ID', 'R2': -11.599, 'RMSE': 0.302, 'MAE': 0.279, 'STD': 0.247, '0R2': 0.31, '0RMSE': 0.138, 'R2_INV': 0.921658370720488}, {'Extract': 0.5, 'Samples location': 'mean', 'Interface': 'observed', 'Forward_Model': 'FSeq', 'Minimization_Method': 'Gauss-Newton', 'Alpha': 0.01, 'remove_coil': False, 'start_avg': True, 'constrain': True, 'Det': 'LS', 'R2': -8.701, 'RMSE': 0.266, 'MAE': 0.232, 'STD': 0.228, '0R2': 0.145, '0RMSE': 0.158, 'R2_INV': 0.921658370720488}]\n",
      "Iteration: 5 Combination: (0.5, 'mean', 'observed', 'FSeq', 'Gauss-Newton', 0.01, False, True, False)\n",
      "#################################################################################################################################################################### 03 DETERMINISTIC MODELLING ###################################################\n",
      "inv_columns Index(['EC_0.30', 'EC_0.60', 'EC_1.00', 'EC_2.00', 'EC_end'], dtype='object')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_74348\\2896892183.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m     81\u001b[0m                 \u001b[0mSA_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSA_file_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m                 \u001b[1;31m#os.remove(file_path_FAILED)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m                 \u001b[0mnew_fine\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m             \u001b[1;31m# Create a dictionary for the current iteration's results\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m             current_results = [{'Extract': extract, 'Samples location': sample_loc, 'Interface': interface, 'Forward_Model': FM, 'Minimization_Method': MinM, 'Alpha': alpha,\n\u001b[0;32m     88\u001b[0m                                 \u001b[1;34m'remove_coil'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mremove_coil\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'start_avg'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstart_avg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'constrain'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mconstrain\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\mendo\\OneDrive - UGent\\Documentos\\PhD\\EM case survey\\EMI_survey_code\\SA_all.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(site, extract, sample_loc, interface, FM, MinM, alpha, remove_coil, start_avg, constrain)\u001b[0m\n\u001b[0;32m   1208\u001b[0m     \u001b[0mfeature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'bulk_ec_inv'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1209\u001b[0m     \u001b[1;31m#Dresults = {}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1210\u001b[0m     \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'vwc'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1212\u001b[1;33m     \u001b[0mDR2_LT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDRMSE_LT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDR2_ID\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDRMSE_ID\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDR2_LS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDRMSE_LS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDR2_10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDRMSE_10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDR2_50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDRMSE_50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mD0R2_LT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mD0RMSE_LT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mD0R2_ID\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mD0RMSE_ID\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mD0R2_LS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mD0RMSE_LS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMAE_LS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMAE_LT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMAE_10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMAE_50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMAE_ID\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSTD_LS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSTD_LT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSTD_10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSTD_50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSTD_ID\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf_ec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclay_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbd_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwater_ec_hp_mean_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_mean_conv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclay_10cm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbd_10cm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwater_ec_hp_10cm_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_10cm_conv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclay_50cm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbd_50cm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwater_ec_hp_50cm_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_50cm_conv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_conv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1213\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1214\u001b[0m     \u001b[0mround_n\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1215\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\mendo\\OneDrive - UGent\\Documentos\\PhD\\EM case survey\\EMI_survey_code\\SA_functions.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(feature, target, df, Y0, f_ec, clay_mean, bd_mean, water_ec_hp_mean_t, t_mean_conv, clay_10cm, bd_10cm, water_ec_hp_10cm_t, t_10cm_conv, clay_50cm, bd_50cm, water_ec_hp_50cm_t, t_50cm_conv, t_conv, iters, round_n)\u001b[0m\n\u001b[0;32m    149\u001b[0m                     \u001b[0mbulk_density\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbd_10cm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m                     \u001b[0mwater_ec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwater_ec_hp_10cm_t\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m                     \u001b[0mtemperature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt_10cm_conv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m                     )\n\u001b[1;32m--> 153\u001b[1;33m         \u001b[0mDypred_10\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWater\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer_10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m         \u001b[0mDR2_10\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr2_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDypred_10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mround_n\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m         \u001b[0mDRMSE_10\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRMSE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDypred_10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mround_n\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m         \u001b[0mMAE_10\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmean_absolute_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDypred_10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mround_n\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\mendo\\OneDrive - UGent\\Documentos\\PhD\\EM case survey\\EMI_survey_code\\pedophysics\\predict\\water.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(soil)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[1;31m# Condition to obtain water from bulk_ec_dc_tc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[0mFrequencyEC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoil\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[0mshift_to_bulk_ec_dc_tc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoil\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwater\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbulk_ec_dc_tc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m         \u001b[0mWaterFromEC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoil\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[1;31m# Converting negative results due to fitting to zero\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m     soil.info['water'] = [str(soil.info.water[x]) + \"--> Set to 0 because of < 0 results\" if soil.df.water[x]<0 \n",
      "\u001b[1;32mc:\\Users\\mendo\\OneDrive - UGent\\Documentos\\PhD\\EM case survey\\EMI_survey_code\\pedophysics\\predict\\water_from_ec.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(soil)\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[0mfitting\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoil\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[1;31m# Check for conditions to use a non-fitting approach\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwater\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbulk_ec_dc_tc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[0mnon_fitting\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoil\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\mendo\\OneDrive - UGent\\Documentos\\PhD\\EM case survey\\EMI_survey_code\\pedophysics\\predict\\water_from_ec.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(soil)\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[0mWaterEC\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mCompute\u001b[0m \u001b[0mmissing\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0mof\u001b[0m \u001b[0msoil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwater_ec\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m     \u001b[0mSolidEC\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mSet\u001b[0m \u001b[0mmissing\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0mof\u001b[0m \u001b[0msoil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msolid_ec\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m     \"\"\"    \n\u001b[0;32m    116\u001b[0m     \u001b[0mTexture\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoil\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m     \u001b[0mPorosity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoil\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m     \u001b[0mWaterEC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoil\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m     \u001b[0mSolidEC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoil\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\mendo\\OneDrive - UGent\\Documentos\\PhD\\EM case survey\\EMI_survey_code\\pedophysics\\predict\\porosity.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(soil)\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;31m# Assuming `soil` is a pre-defined Soil object with the required attributes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[0mporosity_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPorosity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoil\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mporosity_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \"\"\"\n\u001b[1;32m---> 40\u001b[1;33m     \u001b[0mParticleDensity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoil\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m     \u001b[1;31m# Check if any value of porosity is missing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mporosity\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbulk_density\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\mendo\\OneDrive - UGent\\Documentos\\PhD\\EM case survey\\EMI_survey_code\\pedophysics\\predict\\particle_density.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(soil)\u001b[0m\n\u001b[0;32m     56\u001b[0m     \"\"\"\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;31m# Check if any value of particle_density is missing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparticle_density\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m         \u001b[0mTexture\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoil\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         soil.info['particle_density'] = [\"Calculated using Schjonnen function (RMSE = 0.011 g/cm3)\" if np.isnan(soil.df.particle_density[x]) \n\u001b[0;32m     63\u001b[0m                                          \u001b[1;32mor\u001b[0m \u001b[0msoil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparticle_density\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"Calculated using Schjonnen function (RMSE = 0.011 g/cm3)\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\mendo\\OneDrive - UGent\\Documentos\\PhD\\EM case survey\\EMI_survey_code\\pedophysics\\predict\\texture.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(soil)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m     \u001b[1;31m# Go over each texture and assign the corresponding fractions where needed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mtexture\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfractions\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtexture_to_fractions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         \u001b[0msoil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sand'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'silt'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'clay'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msoil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtexture\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mtexture\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'sand'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'silt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'clay'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'Fraction calculated using soil.texture'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Fraction calculated using soil.texture'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Fraction calculated using soil.texture'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m         \u001b[0msoil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sand'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'silt'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'clay'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msoil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtexture\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mtexture\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'sand'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'silt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'clay'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfractions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\mendo\\anaconda3_2\\Lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m    907\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_setitem_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    908\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_has_valid_setitem_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    909\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    910\u001b[0m         \u001b[0miloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"iloc\"\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 911\u001b[1;33m         \u001b[0miloc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_setitem_with_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\mendo\\anaconda3_2\\Lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, indexer, value, name)\u001b[0m\n\u001b[0;32m   1938\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1939\u001b[0m         \u001b[1;31m# align and set the values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1940\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtake_split_path\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1941\u001b[0m             \u001b[1;31m# We have to operate column-wise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1942\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_setitem_with_indexer_split_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1943\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1944\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_setitem_single_block\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\mendo\\anaconda3_2\\Lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, indexer, value, name)\u001b[0m\n\u001b[0;32m   2012\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2013\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0milocs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2014\u001b[0m                 \u001b[1;31m# We are setting multiple columns in a single row.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2015\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mloc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0milocs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2016\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_setitem_single_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2017\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2018\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0milocs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_null_slice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2019\u001b[0m                 \u001b[1;31m# This is a setitem-with-expansion, see\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\mendo\\anaconda3_2\\Lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, loc, value, plane_indexer)\u001b[0m\n\u001b[0;32m   2173\u001b[0m                     \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2174\u001b[0m                 )\n\u001b[0;32m   2175\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumn_setitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplane_indexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2177\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_clear_item_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\mendo\\anaconda3_2\\Lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   4611\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_clear_item_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4612\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_item_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define the variables and their possible values\n",
    "site = 'P'\n",
    "\n",
    "extracts = [0.5, 2.5]\n",
    "sample_locs = ['mean', 'closest']\n",
    "interfaces = ['observed', 'log']\n",
    "FMs = ['FSeq', 'CS', 'FSlin']\n",
    "MinMs = ['Gauss-Newton', 'ROPE']\n",
    "alphas = [0.01, 0.07, 0.2]\n",
    "remove_coils = [True, False]\n",
    "start_avgs = [True, False]\n",
    "constrains = [True, False]\n",
    "\n",
    "SA_results = 'SA_results/'\n",
    "file_path_all = 'dt'+site+'_'+str(extracts)+'_'+str(sample_locs)+'_'+str(['obs', 'log'])+'_'+str(FMs)+'_'+str(['GN', 'ROPE'])+'_'+str(alphas)+'_'+str(['T', 'F'])+'_'+str(['T', 'F'])+'_'+str(['T', 'F'])+'_det'\n",
    "\n",
    "# Initialize DataFrame\n",
    "dt = pd.DataFrame()\n",
    "i = 0  # to keep track of iterations for saving purposes\n",
    "exist_fine = 0 \n",
    "exist_failed = 0\n",
    "new_fine = 0\n",
    "new_failed = 0\n",
    "exist_crash = 0\n",
    "new_crash = 0\n",
    "\n",
    "# Iterate over all combinations\n",
    "for combination in itertools.product(extracts, sample_locs, interfaces, FMs, MinMs, alphas, remove_coils, start_avgs, constrains):\n",
    "\n",
    "    extract, sample_loc, interface, FM, MinM, alpha, remove_coil, start_avg, constrain = combination\n",
    "    print('Iteration:', i, 'Combination:', combination)\n",
    "\n",
    "    file_path = 'dt'+site+'_'+str(extract)+'_'+str(sample_loc)+'_'+str(interface)+'_'+str(FM)+'_'+str(MinM)+'_'+str(alpha)+'_'+str(remove_coil)+'_'+str(start_avg)+'_'+str(constrain)+'_det'\n",
    "    SA_file_path = SA_results + file_path+'.csv'\n",
    "    file_path_FAILED = SA_results + file_path+'_FAILED'+'.csv'\n",
    "    file_path_CRASH = SA_results + file_path+'_CRASH'+'.csv'\n",
    "\n",
    "    if os.path.exists(SA_file_path):\n",
    "        print('exists')\n",
    "        SA_file = pd.read_csv(SA_file_path)\n",
    "        exist_fine += 1\n",
    "\n",
    "    elif os.path.exists(file_path_FAILED):\n",
    "        print('exists but failed')\n",
    "        SA_file = pd.read_csv(file_path_FAILED)\n",
    "        exist_failed += 1\n",
    "\n",
    "    elif os.path.exists(file_path_CRASH):\n",
    "        print('exists but crashed')\n",
    "        SA_file = pd.read_csv(file_path_CRASH)\n",
    "        exist_crash += 1\n",
    "\n",
    "    else:\n",
    "        try:\n",
    "            # Call the SA function with the current combination\n",
    "            results = SA(site, extract, sample_loc, interface, FM, MinM, alpha, remove_coil, start_avg, constrain)\n",
    "\n",
    "            if results is None:\n",
    "                # Create a dictionary for the current iteration's results\n",
    "                current_results = [{'Extract': extract, 'Samples location': sample_loc, 'Interface': interface, 'Forward_Model': FM, 'Minimization_Method': MinM, 'Alpha': alpha,\n",
    "                                    'remove_coil': remove_coil, 'start_avg': start_avg, 'constrain': constrain,\n",
    "                                'Det': det, 'R2': np.nan, 'RMSE': np.nan, 'MAE': np.nan, 'STD': np.nan, '0R2': np.nan, '0RMSE': np.nan, 'R2_INV': np.nan}\n",
    "                            for det, res in zip(['LT', 'ID', 'LS'], [(np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan) for i in range(0, 6, 2)])]\n",
    "                \n",
    "                print('FAILED')\n",
    "                SA_file = pd.DataFrame(current_results)\n",
    "                #print(f\"An error occurred: {e} with combination {combination}\")\n",
    "                SA_file.to_csv(file_path_FAILED, index=False)\n",
    "                new_failed += 1\n",
    "\n",
    "            else: \n",
    "                # Create a dictionary for the current iteration's results\n",
    "                current_results = [{'Extract': extract, 'Samples location': sample_loc, 'Interface': interface, 'Forward_Model': FM, 'Minimization_Method': MinM, 'Alpha': alpha,\n",
    "                                    'remove_coil': remove_coil, 'start_avg': start_avg, 'constrain': constrain,\n",
    "                                    'Det': det, 'R2': res[0], 'RMSE': res[1], 'MAE': res[2], 'STD': res[3], '0R2': res[4], '0RMSE': res[5], 'R2_INV': res[6]}\n",
    "                                for det, res in zip(['LT', 'ID', 'LS'], [(results[i], results[i+1], results[i+16], results[i+17], results[i+10], results[i+11], results[-1]) for i in range(0, 6, 2)])]\n",
    "\n",
    "                print('current_results', current_results)\n",
    "                SA_file = pd.DataFrame(current_results)\n",
    "                # Save the DataFrame to CSV after each iteration\n",
    "                SA_file.to_csv(SA_file_path, index=False)\n",
    "                #os.remove(file_path_FAILED)\n",
    "                new_fine += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            # Create a dictionary for the current iteration's results\n",
    "            current_results = [{'Extract': extract, 'Samples location': sample_loc, 'Interface': interface, 'Forward_Model': FM, 'Minimization_Method': MinM, 'Alpha': alpha,\n",
    "                                'remove_coil': remove_coil, 'start_avg': start_avg, 'constrain': constrain,\n",
    "                                'Det': det, 'R2': np.nan, 'RMSE': np.nan, 'MAE': np.nan, 'STD': np.nan, '0R2': np.nan, '0RMSE': np.nan, 'R2_INV': np.nan}\n",
    "                            for det, res in zip(['LT', 'ID', 'LS'], [(np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan) for i in range(0, 6, 2)])]\n",
    "            \n",
    "            print('CRASH')\n",
    "            SA_file = pd.DataFrame(current_results)\n",
    "            #print(f\"An error occurred: {e} with combination {combination}\")\n",
    "            SA_file.to_csv(file_path_CRASH, index=False)\n",
    "            new_crash += 1\n",
    "\n",
    "            continue  # Continue to the next iteration even if an error occurs\n",
    "\n",
    "    #print('SA_file.head()', SA_file.head())\n",
    "    #print('dt', dt.head())\n",
    "    # Append current results to the DataFrame\n",
    "    dt = pd.concat([dt, SA_file])\n",
    "    i += 1  # Increment the iteration counter\n",
    "\n",
    "print('exist_fine', exist_fine)\n",
    "print('exist_CRASH', exist_crash)\n",
    "print('new_fine', new_fine)\n",
    "print('new_CRASH', new_crash)\n",
    "\n",
    "print(dt)\n",
    "dt.to_csv(SA_results + file_path_all+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SA_all import SA\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from plots import *\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Define the variables and their possible values\n",
    "site = 'P'\n",
    "\n",
    "extracts = [0.5, 2.5]\n",
    "sample_locs = ['mean', 'closest']\n",
    "interfaces = ['observed', 'log']\n",
    "FMs = ['FSeq', 'CS', 'FSlin']\n",
    "MinMs = ['Gauss-Newton', 'ROPE']\n",
    "alphas = [0.01, 0.07, 0.2]\n",
    "remove_coils = [True, False]\n",
    "start_avgs = [True, False]\n",
    "constrains = [True, False]\n",
    "\n",
    "SA_results = 'SA_results/'\n",
    "file_path_all = 'dt'+site+'_'+str(extracts)+'_'+str(sample_locs)+'_'+str(['obs', 'log'])+'_'+str(FMs)+'_'+str(['GN', 'ROPE'])+'_'+str(alphas)+'_'+str(['T', 'F'])+'_'+str(['T', 'F'])+'_'+str(['T', 'F'])+'_det'\n",
    "\n",
    "# Initialize DataFrame\n",
    "dt = pd.DataFrame()\n",
    "i = 0  # to keep track of iterations for saving purposes\n",
    "exist_fine = 0 \n",
    "exist_failed = 0\n",
    "new_fine = 0\n",
    "new_failed = 0\n",
    "exist_crash = 0\n",
    "new_crash = 0\n",
    "\n",
    "# Iterate over all combinations\n",
    "for combination in itertools.product(extracts, sample_locs, interfaces, FMs, MinMs, alphas, remove_coils, start_avgs, constrains):\n",
    "\n",
    "    extract, sample_loc, interface, FM, MinM, alpha, remove_coil, start_avg, constrain = combination\n",
    "    print('Iteration:', i, 'Combination:', combination)\n",
    "\n",
    "    file_path = 'dt'+site+'_'+str(extract)+'_'+str(sample_loc)+'_'+str(interface)+'_'+str(FM)+'_'+str(MinM)+'_'+str(alpha)+'_'+str(remove_coil)+'_'+str(start_avg)+'_'+str(constrain)+'_det'\n",
    "    SA_file_path = SA_results + file_path+'.csv'\n",
    "    file_path_FAILED = SA_results + file_path+'_FAILED'+'.csv'\n",
    "    file_path_CRASH = SA_results + file_path+'_CRASH'+'.csv'\n",
    "\n",
    "    if os.path.exists(SA_file_path):\n",
    "        print('exists')\n",
    "        SA_file = pd.read_csv(SA_file_path)\n",
    "        exist_fine += 1\n",
    "\n",
    "    elif os.path.exists(file_path_FAILED):\n",
    "        print('exists but failed')\n",
    "        SA_file = pd.read_csv(file_path_FAILED)\n",
    "        exist_failed += 1\n",
    "\n",
    "    elif os.path.exists(file_path_CRASH):\n",
    "        print('exists but crashed')\n",
    "        SA_file = pd.read_csv(file_path_CRASH)\n",
    "        exist_crash += 1\n",
    "\n",
    "    else:\n",
    "            # Call the SA function with the current combination\n",
    "            results = SA(site, extract, sample_loc, interface, FM, MinM, alpha, remove_coil, start_avg, constrain)\n",
    "\n",
    "            if results is None:\n",
    "                # Create a dictionary for the current iteration's results\n",
    "                current_results = [{'Extract': extract, 'Samples location': sample_loc, 'Interface': interface, 'Forward_Model': FM, 'Minimization_Method': MinM, 'Alpha': alpha,\n",
    "                                    'remove_coil': remove_coil, 'start_avg': start_avg, 'constrain': constrain,\n",
    "                                'Det': det, 'R2': np.nan, 'RMSE': np.nan, 'MAE': np.nan, 'STD': np.nan, '0R2': np.nan, '0RMSE': np.nan, 'R2_INV': np.nan}\n",
    "                            for det, res in zip(['LT', 'ID', 'LS'], [(np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan) for i in range(0, 6, 2)])]\n",
    "                \n",
    "                print('FAILED')\n",
    "                SA_file_FAILED = pd.DataFrame(current_results)\n",
    "                #print(f\"An error occurred: {e} with combination {combination}\")\n",
    "                SA_file_FAILED.to_csv(file_path_FAILED, index=False)\n",
    "                new_failed += 1\n",
    "\n",
    "            else: \n",
    "                # Create a dictionary for the current iteration's results\n",
    "                current_results = [{'Extract': extract, 'Samples location': sample_loc, 'Interface': interface, 'Forward_Model': FM, 'Minimization_Method': MinM, 'Alpha': alpha,\n",
    "                                    'remove_coil': remove_coil, 'start_avg': start_avg, 'constrain': constrain,\n",
    "                                    'Det': det, 'R2': res[0], 'RMSE': res[1], 'MAE': res[2], 'STD': res[3], '0R2': res[4], '0RMSE': res[5], 'R2_INV': res[6]}\n",
    "                                for det, res in zip(['LT', 'ID', 'LS'], [(results[i], results[i+1], results[i+16], results[i+17], results[i+10], results[i+11], results[-1]) for i in range(0, 6, 2)])]\n",
    "\n",
    "                print('current_results', current_results)\n",
    "                SA_file = pd.DataFrame(current_results)\n",
    "                # Save the DataFrame to CSV after each iteration\n",
    "                SA_file.to_csv(SA_file_path, index=False)\n",
    "                #os.remove(file_path_FAILED)\n",
    "                new_fine += 1\n",
    "\n",
    "    #print('SA_file.head()', SA_file.head())\n",
    "    #print('dt', dt.head())\n",
    "    # Append current results to the DataFrame\n",
    "    dt = pd.concat([dt, SA_file])\n",
    "    i += 1  # Increment the iteration counter\n",
    "\n",
    "print('exist_fine', exist_fine)\n",
    "print('exist_CRASH', exist_crash)\n",
    "print('new_fine', new_fine)\n",
    "print('new_CRASH', new_crash)\n",
    "\n",
    "print(dt)\n",
    "dt.to_csv(SA_results + file_path_all+'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SA_all import SA\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from plots import *\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Define the variables and their possible values\n",
    "site = 'M'\n",
    "\n",
    "clls = [0.2, 0.3]\n",
    "percents = [10, 20, 30]\n",
    "sample_locs = ['mean', 'closest']\n",
    "interfaces = ['observed', 'log-defined']\n",
    "FMs = ['FSeq', 'CS', 'FSlin']\n",
    "MinMs = ['Gauss-Newton', 'ROPE']\n",
    "alphas = [0.01, 0.07, 0.2]\n",
    "remove_coils = [True, False]\n",
    "start_avgs = [True, False]\n",
    "constrains = [True, False]\n",
    "\n",
    "SA_results = 'SA_results/'\n",
    "\n",
    "file_path_all = 'dt'+site+'_'+str(clls)+'_'+str(percents)+'_'+str(sample_locs)+'_'+str(['obs', 'logd'])+'_'+str(FMs)+'_'+str(['GN', 'ROPE'])+'_'+str(alphas)+'_'+str(['T', 'F'])+'_'+str(['T', 'F'])+'_'+str(['T', 'F'])+'_det'\n",
    "\n",
    "# Initialize DataFrame\n",
    "dt = pd.DataFrame()\n",
    "i = 0  # to keep track of iterations for saving purposes\n",
    "exist_fine = 0 \n",
    "exist_failed = 0\n",
    "new_fine = 0\n",
    "new_failed = 0\n",
    "\n",
    "# Iterate over all combinations\n",
    "for combination in itertools.product(clls, percents, sample_locs, interfaces, FMs, MinMs, alphas, remove_coils, start_avgs, constrains):\n",
    "\n",
    "    cl, percent, sample_loc, interface, FM, MinM, alpha, remove_coil, start_avg, constrain = combination\n",
    "    print('Iteration:', i, 'Combination:', combination)\n",
    "\n",
    "    file_path = 'dt'+site+'_'+str(cl)+'_'+str(percent)+'_'+str(sample_loc)+'_'+str(interface)+'_'+str(FM)+'_'+str(MinM)+'_'+str(alpha)+'_'+str(remove_coil)+'_'+str(start_avg)+'_'+str(constrain)+'_det'\n",
    "    SA_file_path = SA_results + file_path+'.csv'\n",
    "    file_path_FAILED = SA_results + file_path+'_FAILED'+'.csv'\n",
    "\n",
    "    if os.path.exists(SA_file_path):\n",
    "        print('exists')\n",
    "        SA_file = pd.read_csv(SA_file_path)\n",
    "        exist_fine += 1\n",
    "\n",
    "    elif os.path.exists(file_path_FAILED):\n",
    "        print('exists but failed')\n",
    "        exist_failed += 1\n",
    "\n",
    "    else:\n",
    "        try:\n",
    "            # Call the SA function with the current combination\n",
    "            results = SA(site, cl, percent, sample_loc, interface, FM, MinM, alpha, remove_coil, start_avg, constrain)\n",
    "\n",
    "            # Create a dictionary for the current iteration's results\n",
    "            current_results = [{'cl': cl, 'percent': percent, 'Samples location': sample_loc, 'Interface': interface, 'Forward_Model': FM, 'Minimization_Method': MinM, 'Alpha': alpha,\n",
    "                                'remove_coil': remove_coil, 'start_avg': start_avg, 'constrain': constrain,\n",
    "                                'Det': det, 'R2': res[0], 'RMSE': res[1], '0R2': res[2], '0RMSE': res[3]}\n",
    "                            for det, res in zip(['LT', 'ID', 'LS'], [(results[i], results[i+1], results[i+10], results[i+11]) for i in range(0, 6, 2)])]\n",
    "\n",
    "            print('current_results', current_results)\n",
    "            SA_file = pd.DataFrame(current_results)\n",
    "            # Save the DataFrame to CSV after each iteration\n",
    "            SA_file.to_csv(SA_file_path, index=False)\n",
    "            #os.remove(file_path_FAILED)\n",
    "            new_fine += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            # Create a dictionary for the current iteration's results\n",
    "            current_results = [{'cl': cl, 'percent': percent, 'Samples location': sample_loc, 'Interface': interface, 'Forward_Model': FM, 'Minimization_Method': MinM, 'Alpha': alpha,\n",
    "                                'remove_coil': remove_coil, 'start_avg': start_avg, 'constrain': constrain,\n",
    "                                'Det': det, 'R2': np.nan, 'RMSE': np.nan, '0R2': np.nan, '0RMSE': np.nan}\n",
    "                            for det, res in zip(['LT', 'ID', 'LS'], [(np.nan, np.nan, np.nan, np.nan) for i in range(0, 6, 2)])]\n",
    "            \n",
    "            print('FAILED')\n",
    "            SA_file = pd.DataFrame(current_results)\n",
    "            #print(f\"An error occurred: {e} with combination {combination}\")\n",
    "            SA_file_FAILED.to_csv(file_path_FAILED, index=False)\n",
    "            new_failed += 1\n",
    "\n",
    "            continue  # Continue to the next iteration even if an error occurs\n",
    "\n",
    "    #print('SA_file.head()', SA_file.head())\n",
    "    #print('dt', dt.head())\n",
    "    # Append current results to the DataFrame\n",
    "    dt = pd.concat([dt, SA_file])\n",
    "    i += 1  # Increment the iteration counter\n",
    "\n",
    "print('exist_fine', exist_fine)\n",
    "print('exist_failed', exist_failed)\n",
    "print('new_fine', new_fine)\n",
    "print('new_failed', new_failed)\n",
    "\n",
    "print(dt)\n",
    "dt.to_csv(SA_results + file_path_all+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import numpy as np\n",
    "from plots import *\n",
    "from plots import *\n",
    "indicator = 'RMSE'\n",
    "\n",
    "extract = [0.5, 2.5]\n",
    "sample_locs = ['mean', 'closest']\n",
    "interfaces = ['observed', 'log-defined']\n",
    "FMs = ['FSeq', 'CS', 'FSlin']\n",
    "MinMs = ['Gauss-Newton', 'ROPE']\n",
    "alphas = [0.01, 0.07, 0.2]\n",
    "remove_coils = [True, False]\n",
    "start_avgs = [True, False]\n",
    "constrains = [True, False]\n",
    "\n",
    "SA_results = 'SA_results/'\n",
    "\n",
    "# Example usage\n",
    "file_path_all_ = str(extract)+'_'+str(sample_locs)+'_'+str(['obs', 'log'])+'_'+str(FMs)+'_'+str(['GN', 'ROPE'])+'_'+str(alphas)+'_'+str(['T', 'F'])+'_'+str(['T', 'F'])+'_'+str(['T', 'F'])+'_det'\n",
    "\n",
    "SA_plot(file_path_all_, SA_results, indicator)\n",
    "\n",
    "\n",
    "dtM = pd.read_csv(SA_results + 'dt' + 'M' + '_' + file_path_all_ + '.csv')\n",
    "\n",
    "print(np.nanmean(dtM.R2_INV[dtM.Minimization_Method == 'Gauss-Newton'].values))\n",
    "print(np.nanmean(dtM.R2_INV[(dtM.Minimization_Method == 'Gauss-Newton') & (dtM.remove_coil == True)].values))\n",
    "print(np.nanmean(dtM.R2_INV[(dtM.Minimization_Method == 'Gauss-Newton') & (dtM.remove_coil == False)].values))\n",
    "\n",
    "print(dtM.R2_INV[dtM.Minimization_Method == 'Gauss-Newton'].isna().sum())\n",
    "print(len(dtM.R2_INV[dtM.Minimization_Method == 'Gauss-Newton'].values))\n",
    "\n",
    "print('Nan mean of R2 M ROPE', np.nanmean(dtM.R2_INV[dtM.Minimization_Method == 'ROPE'].values))\n",
    "print('Crashed M ROPE', dtM.R2_INV[dtM.Minimization_Method == 'ROPE'].isna().sum(), ' of ', len(dtM.R2_INV[dtM.Minimization_Method == 'ROPE'].values))\n",
    "print('Max of R2 in M', np.nanmax(dtM.R2.values), 'Min of RMSE in M', np.nanmin(dtM.RMSE.values))\n",
    "\n",
    "\n",
    "\n",
    "dtP = pd.read_csv(SA_results + 'dt' + 'P' + '_' + file_path_all_ + '.csv')\n",
    "\n",
    "print(np.nanmean(dtP.R2_INV[dtP.Minimization_Method == 'Gauss-Newton'].values))\n",
    "print(np.nanmean(dtP.R2_INV[(dtP.Minimization_Method == 'Gauss-Newton') & (dtP.remove_coil == True)].values))\n",
    "print(np.nanmean(dtP.R2_INV[(dtP.Minimization_Method == 'Gauss-Newton') & (dtP.remove_coil == False)].values))\n",
    "\n",
    "print(dtP.R2_INV[dtP.Minimization_Method == 'Gauss-Newton'].isna().sum())\n",
    "print(len(dtP.R2_INV[dtP.Minimization_Method == 'Gauss-Newton'].values))\n",
    "\n",
    "print('Nan mean of R2 P ROPE', np.nanmean(dtP.R2_INV[dtP.Minimization_Method == 'ROPE'].values))\n",
    "print('Crashed P ROPE', dtP.R2_INV[dtP.Minimization_Method == 'ROPE'].isna().sum(), ' of ', len(dtP.R2_INV[dtP.Minimization_Method == 'ROPE'].values))\n",
    "print('Max of R2 in P', np.nanmax(dtP.R2.values), 'Min of RMSE in P', np.nanmin(dtP.RMSE.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from plots import *\n",
    "from plots import *\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Define the variables and their possible values\n",
    "site = 'M'\n",
    "\n",
    "clls = [0.2, 0.3, 0.4]\n",
    "percents = [10, 20, 30]\n",
    "sample_locs = ['mean', 'closest']\n",
    "#interfaces = ['observed', 'log-defined']\n",
    "FMs = ['FSeq', 'CS', 'FSlin']\n",
    "MinMs = ['Gauss-Newton', 'ROPE']\n",
    "alphas = [0.01, 0.07, 0.2]\n",
    "remove_coils = [True, False]\n",
    "start_avgs = [True, False]\n",
    "constrains = [True, False]\n",
    "\n",
    "SA_results = 'SA_results/'\n",
    "\n",
    "i = 0  # to keep track of iterations for saving purposes\n",
    "# Iterate over all combinations\n",
    "for combination in itertools.product(clls, percents, sample_locs, FMs, MinMs, alphas, remove_coils, start_avgs, constrains):\n",
    "\n",
    "    cl, percent, sample_loc, FM, MinM, alpha, remove_coil, start_avg, constrain = combination\n",
    "    print('Iteration:', i, 'Combination:', combination)\n",
    "\n",
    "    file_path= 'dt'+site+'_'+str(cl)+'_'+str(percent)+'_'+str(sample_loc)+'_Observed'+'_'+str(FM)+'_'+str(MinM)+'_'+str(alpha)+'_'+str(remove_coil)+'_'+str(start_avg)+'_'+str(constrain)+'_det'    \n",
    "    file_path_new = 'dt'+site+'_'+str(cl)+'_'+str(percent)+'_'+str(sample_loc)+'_observed'+'_'+str(FM)+'_'+str(MinM)+'_'+str(alpha)+'_'+str(remove_coil)+'_'+str(start_avg)+'_'+str(constrain)+'_det'    \n",
    "    SA_file_path = SA_results + file_path+'.csv'\n",
    "    file_path_FAILED = SA_results + file_path+'_FAILED'+'.csv'\n",
    "\n",
    "    SA_file_path_new = SA_results + file_path_new+'.csv'\n",
    "    file_path_FAILED_new = SA_results + file_path_new+'_FAILED'+'.csv'\n",
    "\n",
    "    if os.path.exists(SA_file_path):\n",
    "        SA_file = pd.read_csv(SA_file_path)\n",
    "        SA_file.to_csv(SA_file_path_new, index=False)\n",
    "\n",
    "    elif os.path.exists(file_path_FAILED):\n",
    "        SA_file_FAILED = pd.read_csv(file_path_FAILED)\n",
    "        SA_file_FAILED.to_csv(file_path_FAILED_new, index=False)\n",
    "\n",
    "    i += 1  # Increment the iteration counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from plots import *\n",
    "from plots import *\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Define the variables and their possible values\n",
    "site = 'M'\n",
    "\n",
    "clls = [0.2, 0.3, 0.4]\n",
    "percents = [10, 20, 30]\n",
    "sample_locs = ['mean', 'closest']\n",
    "#interfaces = ['Observed', 'Log-defined']\n",
    "FMs = ['FSeq', 'CS', 'FSlin']\n",
    "MinMs = ['Gauss-Newton', 'ROPE']\n",
    "alphas = [0.01, 0.07, 0.2]\n",
    "remove_coils = [True, False]\n",
    "start_avgs = [True, False]\n",
    "constrains = [True, False]\n",
    "\n",
    "data_inv_folder = 'data/inverted/'\n",
    "\n",
    "i = 0  # to keep track of iterations for saving purposes\n",
    "# Iterate over all combinations\n",
    "for combination in itertools.product(clls, percents, sample_locs, FMs, MinMs, alphas, remove_coils, start_avgs, constrains):\n",
    "\n",
    "    cl, percent, sample_loc, FM, MinM, alpha, remove_coil, start_avg, constrain = combination\n",
    "    print('Iteration:', i, 'Combination:', combination)\n",
    "\n",
    "    file_path = 'proefhoeve_21HS_inverted_samples'+'_'+str(cl)+'_'+str(percent)+'_'+str(sample_loc)+'_'+str(FM)+'_'+str(MinM)+'_'+str(alpha)+'_'+str(remove_coil)+'_'+str(start_avg)+'_'+str(constrain)\n",
    "    file_path_new = 'proefhoeve_21HS_inverted_samples'+'_'+str(cl)+'_'+str(percent)+'_'+str(sample_loc)+'_Observed'+'_'+str(FM)+'_'+str(MinM)+'_'+str(alpha)+'_'+str(remove_coil)+'_'+str(start_avg)+'_'+str(constrain)\n",
    "\n",
    "    data_inv_path = data_inv_folder + file_path+'.csv'\n",
    "    data_inv_path_new = data_inv_folder + file_path_new+'.csv'\n",
    "\n",
    "    #file_path_FAILED = SA_results + file_path+'_FAILED'+'.csv'\n",
    "\n",
    "    #SA_file_path_new = SA_results + file_path_new+'.csv'\n",
    "    #file_path_FAILED_new = SA_results + file_path_new+'_FAILED'+'.csv'\n",
    "\n",
    "    if os.path.exists(data_inv_path):\n",
    "        inv_file = pd.read_csv(data_inv_path)\n",
    "        inv_file.to_csv(data_inv_path_new, index=False)\n",
    "        i += 1\n",
    "#    elif os.path.exists(file_path_FAILED):\n",
    "#        SA_file_FAILED = pd.read_csv(file_path_FAILED)\n",
    "#        cols = list(SA_file_FAILED.columns)\n",
    "#        cols.remove('Interface')\n",
    "#        cols.insert(3, 'Interface')\n",
    "#        SA_file_FAILED = SA_file_FAILED[cols]\n",
    "#        SA_file_FAILED['Interface'] = 'Observed'\n",
    "#        SA_file_FAILED.to_csv(file_path_FAILED, index=False)\n",
    "\n",
    "#    i += 1  # Increment the iteration counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "# Packages -----------------\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import warnings\n",
    "import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy import constants\n",
    "from matplotlib.path import Path\n",
    "\n",
    "# Get notebook and parent dir\n",
    "current_dir = os.path.dirname(os.path.abspath('__file__')) \n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "\n",
    "# Set path to pedophysics module \n",
    "pedophysics_code_path = os.path.join(parent_dir)\n",
    "sys.path.insert(0, pedophysics_code_path)\n",
    "\n",
    "import pedophysics\n",
    "from pedophysics import predict, Soil\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.optimize import root\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.optimize import minimize, differential_evolution\n",
    "from IPython.display import clear_output\n",
    "from utils.spatial_utils import utm_to_epsg, get_coincident\n",
    "#!pip install pymel\n",
    "import pymel\n",
    "from FDEM import Initialize\n",
    "from utils.profile_utils import merge_layers, plot_profile, check_uniformity_and_interpolate\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import pearsonr\n",
    "from PyPDF2 import PdfMerger\n",
    "from emagpy import Problem\n",
    "\n",
    "# Electromagnetic induction data inversion package\n",
    "from plots import *\n",
    "from PM import *\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.options.future.infer_string = True\n",
    "\n",
    "sys.path.insert(0,'../src/') # this add the emagpy/src directory to the PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pedophysical modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datetime for filename\n",
    "now = (datetime.datetime.now())\n",
    "now = now.strftime(\"%y%m%d_%H%M\")\n",
    "\n",
    "# User input\n",
    "s_site = 'M'; # P = Proefhoeve; M = Middelkerke\n",
    "# Define input datatype and source folder\n",
    "em_intype = 'rec'   # 'rec' = rECa transect; 'lin' = LIN ECa transect; \n",
    "                    # 'survey' = rEC full survey\n",
    "\n",
    "config = {}\n",
    "\n",
    "config['instrument_code'] = 'Dualem-21HS' # instrument code\n",
    "\n",
    "cal = 'calibrated' # 'non_calibrated', 'drift_calibrated'\n",
    "instrument_code = '21HS' # 421S, '21HS'\n",
    "\n",
    "# User input\n",
    "\n",
    "datafolder = 'data' # data folder\n",
    "\n",
    "if s_site == 'P':\n",
    "    profile_prefix = 'proefhoeve'\n",
    "    if config['instrument_code'] == 'Dualem-21HS':\n",
    "        emfile_prefix = 'proefhoeve_21HS'\n",
    "    else: \n",
    "        emfile_prefix = 'proefhoeve_421S'\n",
    "        \n",
    "else:\n",
    "    profile_prefix = 'middelkerke'\n",
    "    emfile_prefix = 'middelkerke_421S'\n",
    "    # check if correct instrument (only 421S data available for Middelkerke)\n",
    "    if config['instrument_code'] == 'Dualem-21HS':\n",
    "        config['instrument_code'] = 'Dualem-421S'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data import dry down experiment\n",
    "dry_down = os.path.join(datafolder, f'Dry_down.csv')\n",
    "dry_d = pd.read_csv(dry_down, sep=',', header=0)\n",
    "\n",
    "cal_folder = os.path.join(datafolder, 'calibrated')\n",
    "em_survey = os.path.join(cal_folder, f'{emfile_prefix}_calibrated_rECa.csv')\n",
    "em_survey = pd.read_csv(em_survey, sep=',', header=0)\n",
    "sampleprop = os.path.join(datafolder, f'{profile_prefix}_soil_analysis.csv')\n",
    "samples_analysis = pd.read_csv(sampleprop, sep=',', header=0)\n",
    "\n",
    "em_sample_prop = get_coincident(em_survey, samples_analysis)\n",
    "ds_c = em_sample_prop.copy()\n",
    "\n",
    "inverted_data = False # Include inverted data or not\n",
    "\n",
    "if inverted_data:\n",
    "    inverted = os.path.join(datafolder, f'{profile_prefix}_inverted_samples_{instrument_code}c.csv')\n",
    "    ds_inv = pd.read_csv(inverted, sep=',', header=0)\n",
    "    print(ds_inv.head())\n",
    "\n",
    "    inv_columns = ds_inv.columns[3:-1]\n",
    "    ds_c[inv_columns] = np.nan\n",
    "\n",
    "    for idc, c in enumerate(inv_columns):\n",
    "\n",
    "        for i in range(len(ds_inv.x)):\n",
    "            ds_c.loc[ds_c.code == i+1, c] = ds_inv.loc[i, c]\n",
    "\n",
    "    def closest_ec(row):\n",
    "        depth = row['depth']\n",
    "        # Filter columns that start with 'EC_' but not 'EC_end'\n",
    "        ec_cols = [col for col in row.index if col.startswith('EC_') and col != 'EC_end']\n",
    "        # Convert the part after 'EC_' to float and calculate the absolute difference with depth\n",
    "        differences = {col: abs(depth/100 - float(col.split('_')[1])) for col in ec_cols}\n",
    "        # Find the column name with the minimum difference\n",
    "        closest_col = min(differences, key=differences.get)\n",
    "        return row[closest_col]\n",
    "\n",
    "\n",
    "    # Apply the function to each row\n",
    "    ds_c['bulk_ec_inv'] = ds_c.apply(closest_ec, axis=1)\n",
    "\n",
    "    #Obtain EC DC TC\n",
    "    ds_c['bulk_ec_dc_tc_inv'] = predict.BulkECDCTC(Soil(temperature = ds_c.temp.values+273.15,\n",
    "                                                        frequency_ec = 9e3,\n",
    "                                                        bulk_ec = ds_c.bulk_ec_inv.values/1000))\n",
    "    # Mean of input inverted EC DC TC values\n",
    "    EC_mean = np.mean(ds_c['bulk_ec_dc_tc_inv'].values) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deterministic modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caclculate Bulk EC from HydraProbe data at 50Mhz\n",
    "offset = 4\n",
    "water_perm = 80\n",
    "ds_c['bulk_ec_hp'] = logsdon(50e6, ds_c.rperm, ds_c.iperm)\n",
    "\n",
    "ds_c['bulk_ec_dc_hp'] = predict.BulkECDC(Soil(frequency_ec = 50e6,\n",
    "                                              bulk_ec = ds_c.bulk_ec_hp.values))\n",
    "\n",
    "ds_c['bulk_ec_tc_hp'] = SheetsHendrickxEC( ds_c.bulk_ec_hp, ds_c.temp)\n",
    "ds_c['bulk_ec_dc_tc_hp'] = predict.BulkECDCTC(Soil(temperature = ds_c.temp.values,\n",
    "                                                    bulk_ec_dc = ds_c.bulk_ec_dc_hp.values\n",
    "                                                    ))\n",
    "\n",
    "# Caclculate Water EC from HydraProbe data at 50Mhz\n",
    "ds_c['water_ec_hp'] = Hilhorst(ds_c.bulk_ec_hp, ds_c.rperm, water_perm, offset)\n",
    "ds_c['water_ec_hp_t'] = WraithOr(ds_c.water_ec_hp, ds_c.temp)\n",
    "ds_c['iperm_water_t'] = ds_c.water_ec_hp_t/(50e6*2*pi*epsilon_0)\n",
    "\n",
    "# -------------------------------------------------------------------------------------\n",
    "\n",
    "# DRY DOWN experiment\n",
    "dry_d['P_top_EC'] = logsdon(50e6, dry_d.P_top_RP, dry_d.P_top_IP)\n",
    "dry_d['P_bot_EC'] = logsdon(50e6, dry_d.P_bot_RP, dry_d.P_bot_IP)\n",
    "dry_d['M_top_EC'] = logsdon(50e6, dry_d.M_top_RP, dry_d.M_top_IP)\n",
    "dry_d['M_bot_EC'] = logsdon(50e6, dry_d.M_bot_RP, dry_d.M_bot_IP)\n",
    "\n",
    "dry_d['P_top_ECw'] = predict.WaterEC(Soil(bulk_ec_dc = dry_d.P_top_EC.values,\n",
    "                                          water = dry_d.P_top_W.values,\n",
    "                                          temperature = dry_d.P_top_T.values+273.15))\n",
    "\n",
    "dry_d['P_bot_ECw'] = predict.WaterEC(Soil(bulk_ec_dc = dry_d.P_bot_EC.values,\n",
    "                                          water = dry_d.P_bot_W.values,\n",
    "                                          temperature = dry_d.P_bot_T.values+273.15))\n",
    "\n",
    "dry_d['M_top_ECw'] = predict.WaterEC(Soil(bulk_ec_dc = dry_d.M_top_EC.values,\n",
    "                                          water = dry_d.M_top_W.values,\n",
    "                                          temperature = dry_d.M_top_T.values+273.15))\n",
    "\n",
    "dry_d['M_bot_ECw'] = predict.WaterEC(Soil(bulk_ec_dc = dry_d.M_bot_EC.values,\n",
    "                                          water = dry_d.M_bot_W.values,\n",
    "                                          temperature = dry_d.M_bot_T.values+273.15))\n",
    "\n",
    "# DRY DOWN experiment\n",
    "dry_d['P_top_EC'] = logsdon(50e6, dry_d.P_top_RP, dry_d.P_top_IP)\n",
    "dry_d['P_bot_EC'] = logsdon(50e6, dry_d.P_bot_RP, dry_d.P_bot_IP)\n",
    "dry_d['M_top_EC'] = logsdon(50e6, dry_d.M_top_RP, dry_d.M_top_IP)\n",
    "dry_d['M_bot_EC'] = logsdon(50e6, dry_d.M_bot_RP, dry_d.M_bot_IP)\n",
    "\n",
    "dry_d['P_top_ECw'] = predict.WaterEC(Soil(bulk_ec_dc = dry_d.P_top_EC.values,\n",
    "                                          water = dry_d.P_top_W.values,\n",
    "                                          temperature = dry_d.P_top_T.values+273.15))\n",
    "\n",
    "dry_d['P_bot_ECw'] = predict.WaterEC(Soil(bulk_ec_dc = dry_d.P_bot_EC.values,\n",
    "                                          water = dry_d.P_bot_W.values,\n",
    "                                          temperature = dry_d.P_bot_T.values+273.15))\n",
    "\n",
    "dry_d['M_top_ECw'] = predict.WaterEC(Soil(bulk_ec_dc = dry_d.M_top_EC.values,\n",
    "                                          water = dry_d.M_top_W.values,\n",
    "                                          temperature = dry_d.M_top_T.values+273.15))\n",
    "\n",
    "dry_d['M_bot_ECw'] = predict.WaterEC(Soil(bulk_ec_dc = dry_d.M_bot_EC.values,\n",
    "                                          water = dry_d.M_bot_W.values,\n",
    "                                          temperature = dry_d.M_bot_T.values+273.15))\n",
    "\n",
    "if profile_prefix == 'proefhoeve':\n",
    "    water_ec_10cm = dry_d.P_top_ECw.values[0]\n",
    "    water_ec_50cm = dry_d.P_bot_ECw.values[0]\n",
    "    water_ec_mean = (water_ec_10cm + water_ec_50cm)/2\n",
    "\n",
    "elif profile_prefix == 'middelkerke':\n",
    "    water_ec_10cm = dry_d.M_top_ECw.values[0]\n",
    "    water_ec_50cm = dry_d.M_bot_ECw.values[0]\n",
    "    water_ec_mean = (water_ec_10cm + water_ec_50cm)/2\n",
    "\n",
    "###################\n",
    "\n",
    "# -------------------------------------------------------------------------------------\n",
    "\n",
    "clay_50cm = np.mean(ds_c.clay[ds_c['depth']==50])\n",
    "clay_10cm = np.mean(ds_c.clay[ds_c['depth']==10])\n",
    "bd_50cm = np.mean(ds_c.bd[ds_c['depth']==50])\n",
    "bd_10cm = np.mean(ds_c.bd[ds_c['depth']==10])\n",
    "water_ec_hp_50cm = np.mean(ds_c.water_ec_hp[ds_c['depth']==50])\n",
    "water_ec_hp_10cm = np.mean(ds_c.water_ec_hp[ds_c['depth']==10])\n",
    "water_ec_hp_50cm_t = np.mean(ds_c.water_ec_hp_t[ds_c['depth']==50])\n",
    "water_ec_hp_10cm_t = np.mean(ds_c.water_ec_hp_t[ds_c['depth']==10])\n",
    "clay_mean = np.mean(ds_c.clay)\n",
    "bd_mean = np.mean(ds_c.bd)\n",
    "water_ec_hp_mean = np.mean(ds_c.water_ec_hp)\n",
    "water_ec_hp_mean_t = np.mean(ds_c.water_ec_hp_t)\n",
    "temp_50cm = np.mean(ds_c.temp[ds_c['depth']==50])\n",
    "temp_10cm = np.mean(ds_c.temp[ds_c['depth']==10])\n",
    "temp_mean = np.mean(ds_c.temp)\n",
    "vwc_50cm = np.mean(ds_c.vwc[ds_c['depth']==50])\n",
    "vwc_10cm = np.mean(ds_c.vwc[ds_c['depth']==10])\n",
    "vwc_mean = np.mean(ds_c.vwc) # 0.289 Proef\n",
    "\n",
    "f_ec = 9000\n",
    "t_conv = 273.15\n",
    "t_mean_conv = temp_mean+t_conv # 297.28 Proef\n",
    "\n",
    "# Mean of observed water values\n",
    "VWC_mean = np.mean(ds_c['vwc'].values) # 0.2891 Proef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted water based on mean input inverted EC DC TC values\n",
    "water_upper = 0.05\n",
    "water_default = 0\n",
    "water_lower = -0.05\n",
    "water_upper_p = 5\n",
    "water_default_p = 0\n",
    "water_lower_p = -5\n",
    "\n",
    "def EC_diff(EC, vwc_diff):\n",
    "\n",
    "    VWC_mean_pred = predict.Water(Soil(\n",
    "                                        bulk_ec = EC,  \n",
    "                                        frequency_ec=f_ec,\n",
    "                                        clay = clay_mean,\n",
    "                                        bulk_density = bd_mean,\n",
    "                                        water_ec = water_ec_mean,\n",
    "                                        temperature = t_mean_conv\n",
    "                                    ))[0] \n",
    "\n",
    "    return ((VWC_mean_pred - VWC_mean) - vwc_diff)**2\n",
    "\n",
    "# EC expected\n",
    "EC_5 = minimize(EC_diff, 0.01, args=(water_upper), bounds= [(0, 1)], method='Nelder-Mead')\n",
    "EC_upper = EC_5.x[0]\n",
    "\n",
    "EC_0 = minimize(EC_diff, 0.01, args=(water_default), bounds= [(0, 1)], method='Nelder-Mead')\n",
    "EC_00 = EC_0.x[0]\n",
    "\n",
    "EC_n5 = minimize(EC_diff, 0.01, args=(water_lower), bounds= [(0, 1)], method='Nelder-Mead')\n",
    "EC_lower = EC_n5.x[0]\n",
    "\n",
    "print(EC_upper, EC_lower, EC_00)\n",
    "\n",
    "# Difference percentual\n",
    "EC_upper_p = 100*(EC_upper - EC_00)/EC_00\n",
    "EC_lower_p = 100*(EC_00 - EC_lower)/EC_00\n",
    "print(EC_upper_p, EC_lower_p)\n",
    "\n",
    "sens_pedm_upper = water_upper_p/EC_upper_p\n",
    "sens_pedm_lower = water_lower_p/EC_lower_p\n",
    "print(sens_pedm_upper, sens_pedm_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_dir = 'temp_emp_04' \n",
    "infile_name = 'infile_s04.csv'\n",
    "os.makedirs(temp_dir, exist_ok=True)\n",
    "temp_file = os.path.join(temp_dir,infile_name)\n",
    "\n",
    "filename = f\"{now}_{emfile_prefix}_parameters_04.json\"\n",
    "filepath = os.path.join(temp_dir,filename)\n",
    "file = open(filepath, 'w')\n",
    "\n",
    "file.write('\\t\"EC_00\":\"{}\",'.format(EC_00) + '\\n')\n",
    "file.write('\\t\"EC_upper_p\":\"{}\",'.format(EC_upper_p) + '\\n')\n",
    "file.write('\\t\"EC_lower_p\":\"{}\",'.format(EC_lower_p) + '\\n')\n",
    "\n",
    "file.write('\\t\"sens_pedm_upper\":\"{}\",'.format(sens_pedm_upper) + '\\n')\n",
    "file.write('\\t\"sens_pedm_lower\":\"{}\",'.format(sens_pedm_lower) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inversion: configure  input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sensor settings\n",
    "config['instrument_code'] = 'Dualem-21HS' # instrument code\n",
    "config['instrument_height'] = 0.165     # instrument height\n",
    "config['instrument_orientation'] = 'HCP'    # instrument orientation\n",
    "\n",
    "# Remove coils for inversion?\n",
    "config['remove_coil'] = True    # set to True if you want to remove coils in the inversion process\n",
    "config['coil_n'] = [4,5]    # indexes of coils to remove (cf. emagpy indexing)\n",
    "                            # for Proefhoeve, coils 0 (HCP05) and 1 (PRP06) are best\n",
    "                            # removed, for Middelkerke coils 4 (HCP4.0) and 5 (PRP4.1)\n",
    "\n",
    "# Inversion parameters\n",
    "config['fs_emp'] = 'FSeq' #'CS', 'FSlin' or 'FSeq'\n",
    "config['opt_method'] = 'L-BFGS-B'  # mMinimize = ['L-BFGS-B','TNC','CG','Nelder-Mead'] --> https://docs.scipy.org/doc/scipy/reference/optimize.html \n",
    "                                # mMCMC = ['ROPE','DREAM', 'MCMC'] # ??? 'SCEUA' ??? --> https://spotpy.readthedocs.io/en/latest/ \n",
    "                                # mOther = ['ANN','Gauss-Newton','GPS'] (ANN requires tensorflow)\n",
    "config['constrain']=True\n",
    "config['regularization'] = 'l2'\n",
    "config['alpha'] = 0.07\n",
    "\n",
    "# Reference profile for starting model (conductivity values)\n",
    "config['start_avg'] = False     # take average of input resistivity profiles per layer as starting model\n",
    "                                # if false, reference profile is taken as starting model\n",
    "\n",
    "# Define the interfaces depths between layers for starting model and inversion\n",
    "#           (number of layers = len(config['interface'])+1)\n",
    "config['n_int'] = True # if True custom interfaces are defined (via config['interface']), \n",
    "                        # otherwise reference profile interfaces are used\n",
    "\n",
    "config['interface'] = [0.3, \n",
    "                       0.6, \n",
    "                       1.0,\n",
    "                       2.0\n",
    "                        ] # depths to custom model interfaces\n",
    "# Inversion constraining\n",
    "\n",
    "config['custom_bounds'] = True\n",
    "config['bounds'] = [(10, 55), (20, 120), (50, 335), (50, 250), (10, 50)]\n",
    "\n",
    "# remove profiles at transect edges\n",
    "config['n_omit'] =  10 # number of profiles to exclude from the start\n",
    "                       # and end of the ERT transect (none = 0) for the inversion\n",
    "                       # a total of 60 profiles is available, for middelkerke\n",
    "                       # 120 profiles are available \n",
    "\n",
    "if config['constrain']:\n",
    "    if config['custom_bounds']:\n",
    "        bounds = config['bounds']\n",
    "\n",
    "if config['n_int'] == False and config['custom_bounds']:\n",
    "    print('Check if bounds and number of interfaces match')\n",
    "\n",
    "# Geographic operations (if needed)\n",
    "c_transform = False\n",
    "c_utmzone = '31N'\n",
    "c_target_cs = 'EPSG:31370'\n",
    "\n",
    "# ---------------------------------------------------------------------------- #\n",
    "# ---------------------------------------------------------------------------- #\n",
    "\n",
    "# Datetime for filename\n",
    "now = (datetime.datetime.now())\n",
    "now = now.strftime(\"%y%m%d_%H%M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------- #\n",
    "# ---------------------------------------------------------------------------- #\n",
    "\n",
    "if s_site == 'P':\n",
    "    profile_prefix = 'proefhoeve'\n",
    "    if config['instrument_code'] == 'Dualem-21HS':\n",
    "        emfile_prefix = 'proefhoeve_21HS'\n",
    "    else: \n",
    "        emfile_prefix = 'proefhoeve_421S'\n",
    "else:\n",
    "    profile_prefix = 'middelkerke'\n",
    "    emfile_prefix = 'middelkerke_421S'\n",
    "    # check if correct instrument (only 421S data available for Middelkerke)\n",
    "    if config['instrument_code'] == 'Dualem-21HS':\n",
    "        config['instrument_code'] = 'Dualem-421S'\n",
    "\n",
    "inv_folder = os.path.join(datafolder, 'inverted')\n",
    "os.makedirs(inv_folder, exist_ok=True) \n",
    "cal_folder = os.path.join(datafolder, 'calibrated')\n",
    "ert_file = os.path.join(datafolder, f'{profile_prefix}-profiles.csv')\n",
    "em_rec = os.path.join(cal_folder, f'{emfile_prefix}_transect_calibrated_rECa.csv')\n",
    "em_lin = os.path.join(cal_folder,f'{emfile_prefix}_transect_calibrated_LIN.csv')\n",
    "em_survey = os.path.join(cal_folder, f'{emfile_prefix}_calibrated_rECa.csv')\n",
    "samplocs = os.path.join(datafolder, f'{profile_prefix}_samps.csv')\n",
    "\n",
    "if em_intype == 'rec':\n",
    "    infile = em_rec\n",
    "elif em_intype == 'survey':\n",
    "    infile = em_survey\n",
    "else:\n",
    "    infile = em_lin\n",
    "\n",
    "instrument = Initialize.Instrument(config['instrument_code'],\n",
    "                                    instrument_height=config['instrument_height'],\n",
    "                                    instrument_orientation=config['instrument_orientation']\n",
    "                                    )\n",
    "\n",
    "# Column names for emapgy input\n",
    "emp_21HS = [f\"HCP0.5f9000{config['instrument_height']}\", 'PRP0.6f9000h0.165', 'HCP1.0f9000h0.165', 'PRP1.1f9000h0.165',\t'HCP2.0f9000h0.165', 'PRP2.1f9000h0.165',\n",
    "            'HCP0.5f9000h0.165_inph', 'PRP0.6f9000h0.165_inph', 'HCP1.0f9000h0.165_inph',\n",
    "            'PRP1.1f9000h0.165_inph', 'HCP2.0f9000h0.165_inph', 'PRP2.1f9000h0.165_inph'\n",
    "            ]\n",
    "\n",
    "emp_421S = ['HCP1.0f9000h0.165', 'PRP1.1f9000h0.165',\t'HCP2.0f9000h0.165', 'PRP2.1f9000h0.165', 'HCP4.0f9000h0.165', 'PRP4.1f9000h0.165', \n",
    "            'HCP1.0f9000h0.165_inph', 'PRP1.1f9000h0.165_inph', 'HCP2.0f9000h0.165_inph', 'PRP2.1f9000h0.165_inph',\n",
    "            'HCP4.0f9000h0.165_inph', 'PRP4.1f9000h0.165_inph',\n",
    "            ]\n",
    "\n",
    "if config['opt_method'] == 'Gauss-Newton':\n",
    "    config['regularization'] = 'l2'\n",
    "\n",
    "# Datetime for filename\n",
    "now = (datetime.datetime.now())\n",
    "now = now.strftime(\"%y%m%d_%H%M\")\n",
    "\n",
    "# 1.0 Data import and structuring into dataframe\n",
    "ert_p = pd.read_csv(ert_file, sep=',', header=0)\n",
    "em_rec = pd.read_csv(em_rec, sep=',', header=0)\n",
    "em_lin = pd.read_csv(em_lin, sep=',', header=0)\n",
    "em_survey = pd.read_csv(em_survey, sep=',', header=0)\n",
    "samples = pd.read_csv(samplocs, sep=',', header=0)\n",
    "\n",
    "\n",
    "if c_transform:\n",
    "    # Create a new filename with the target EPSG code\n",
    "    em_rec = utm_to_epsg(em_rec, c_utmzone, target_epsg=c_target_cs)\n",
    "    em_lin = utm_to_epsg(em_lin, c_utmzone, target_epsg=c_target_cs)\n",
    "    em_survey = utm_to_epsg(em_survey, c_utmzone, target_epsg=c_target_cs)\n",
    "\n",
    "instrument = Initialize.Instrument(config['instrument_code'],\n",
    "                                    instrument_height=config['instrument_height'],\n",
    "                                        instrument_orientation=config['instrument_orientation']\n",
    "                                        )\n",
    "\n",
    "em_samples = get_coincident(em_survey,samples)\n",
    "\n",
    "# ---------------------------------------------------------------------------- #\n",
    "# Get ERT profiles\n",
    "# ---------------- #\n",
    "# Group the data by profile ID for efficient access to each profile\n",
    "profiles = ert_p.groupby('ID')\n",
    "\n",
    "# Exclude the first and last n_omit profiles\n",
    "unique_ids = ert_p['ID'].unique()\n",
    "\n",
    "if config['n_omit'] == 0:\n",
    "    ert_final = ert_p.copy()\n",
    "else:\n",
    "    if config['n_omit']*2 >= len(unique_ids):\n",
    "        warnings.warn('!!! You removed all profiles !!! Change value for config[n_omit]')\n",
    "        raise KeyboardInterrupt\n",
    "    else:\n",
    "        selected_ids = unique_ids[config['n_omit']:-config['n_omit']]\n",
    "        ert_p = ert_p.loc[ert_p['ID'].isin(selected_ids)]\n",
    "        ert_final = ert_p.copy()\n",
    "\n",
    "dataset_name = 'Resistivity(ohm.m)'  # The variable of interest\n",
    "\n",
    "# convert resistivity to conductivity and modify column names\n",
    "\n",
    "ert_final[dataset_name] = (1/ert_final[dataset_name])\n",
    "dc_corr = ert_final.copy()\n",
    "dc_corr[dataset_name] = predict.BulkEC(Soil(\n",
    "                                                frequency_ec = 9000,\n",
    "                                                bulk_ec_dc = dc_corr[dataset_name].values\n",
    "                                                ))\n",
    "\n",
    "ert_final.loc[:, dataset_name] = ert_final[dataset_name]*1000\n",
    "dc_corr.loc[:,dataset_name] = dc_corr[dataset_name]*1000\n",
    "ert_final = ert_final.rename(columns={\"Resistivity(ohm.m)\": \"EC(mS/m)\"})\n",
    "dc_corr = dc_corr.rename(columns={\"Resistivity(ohm.m)\": \"EC(mS/m)\"})\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# Columns containing the resistivity data\n",
    "data_column = ['EC(mS/m)']\n",
    "# Assuming ert_final is your DataFrame with profile data\n",
    "all_profiles_df, uniform_intervals = check_uniformity_and_interpolate(\n",
    "    dc_corr, 'ID', 'z', *data_column\n",
    ")\n",
    "\n",
    "dataset_name = 'EC(mS/m)'  # The variable of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config['reference_profile'] = 11\n",
    "\n",
    "if config['reference_profile'] not in all_profiles_df['ID'].unique():\n",
    "    warnings.warn(\"Warning: the reference profile ID does not exist. Provide correct profile ID.\")\n",
    "    raise KeyboardInterrupt\n",
    "else:\n",
    "    profile_id = config['reference_profile']\n",
    "\n",
    "# Create new layer configuration for prior model based on ERT data\n",
    "if config['n_int']:\n",
    "    new_int = config['interface']\n",
    "    merged_df = merge_layers(all_profiles_df, new_int,'EC(mS/m)')\n",
    "else:\n",
    "    merged_df = all_profiles_df\n",
    "comparedf = merged_df.copy()\n",
    "\n",
    "# Plot original and (merged and) DC corrected reference profile\n",
    "if config['n_int']:\n",
    "    plot_title = 'Original vs merged & DC corrected data'\n",
    "    first_in = .1\n",
    "else: \n",
    "    plot_title = 'Original vs DC corrected data'\n",
    "    first_in = .0\n",
    "ert_eval = ert_final.copy()\n",
    "ert_eval['z'] = ert_eval['z'].values + first_in\n",
    "\n",
    "plot_profile(ert_eval, profile_id, dataset_name, compare=True, compare_df = comparedf, compare_name = 'EC(mS/m)', block=True, plot_title=plot_title)\n",
    "\n",
    "# Get prior model info\n",
    "def generate_forward_model_inputs(df, profile_id_col, depth_col, res_col):\n",
    "    models = {}  # Dictionary to store models by profile ID\n",
    "\n",
    "    for profile_id, group in df.groupby(profile_id_col):\n",
    "        # Assuming uniform interval after previous interpolation\n",
    "        uniform_interval = abs(group[depth_col].diff().iloc[1])\n",
    "        #print(uniform_interval)\n",
    "        num_layers = len(group[res_col])\n",
    "                # Thicknesses are the intervals between depths, except for the last value which does not define a new layer\n",
    "        thick = np.full(num_layers - 1, uniform_interval)\n",
    "        thick[0] = 2 * thick[0]\n",
    "        # Conductivity is the inverse of resistivity\n",
    "        con = group[res_col].values/1000\n",
    "        # Permittivity is the epsilon_0 for all layers\n",
    "        perm = np.full(num_layers, constants.epsilon_0)\n",
    "        sus = np.zeros(num_layers)\n",
    "        # Create model instance\n",
    "        M = Initialize.Model(thick, sus[::-1], con[::-1], perm[::-1])\n",
    "        \n",
    "        # Store the model instance in the dictionary with the profile ID as the key\n",
    "        models[profile_id] = M\n",
    "    return models\n",
    "\n",
    "models = generate_forward_model_inputs(merged_df, 'ID', 'z', 'EC(mS/m)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# -------------------------------------------------------------------- #\n",
    "\n",
    "# \n",
    "profile_data = merged_df[merged_df['ID'] == profile_id].copy()\n",
    "res_col = 'EC(mS/m)'\n",
    "depth = 'z'\n",
    "max_ert_depth = ert_final['z'].abs().max()\n",
    "\n",
    "# \n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# A. Test run on the reference profile (config['reference_profile'])\n",
    "#       and plot the results\n",
    "\n",
    "if not config['n_int']:\n",
    "    first_lay = profile_data[depth].iloc[-1].round(decimals=1)\n",
    "    second_lay = profile_data[depth].iloc[-2].round(decimals=1)\n",
    "    if first_lay == 0:\n",
    "        profile_data[depth]=profile_data[depth] +second_lay\n",
    "    else:\n",
    "        profile_data[depth]=profile_data[depth] +first_lay\n",
    "    thick = -profile_data[depth].iloc[1:].values\n",
    "    #thick = -profile_data[depth].values\n",
    "else:\n",
    "    thick = -profile_data[depth].values\n",
    "\n",
    "con = profile_data[res_col].values/1000\n",
    "ref_len = len(con)\n",
    "num_layers = len(con)\n",
    "perm = np.full(num_layers, constants.epsilon_0)\n",
    "sus = np.zeros(num_layers)\n",
    "\n",
    "# # Create model instance\n",
    "M = Initialize.Model(thick, sus[::-1], con[::-1], perm[::-1])\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "dataset_name = 'EC(mS/m)'\n",
    "layers_interfaces = np.cumsum(models[profile_id].thick)\n",
    "layers_interfaces = np.insert(layers_interfaces, 0, 0)\n",
    "profile_data = ert_final[ert_final['ID'] == profile_id]\n",
    "\n",
    "fig, axr = plt.subplots(figsize=(5, 10))\n",
    "axr.set_xlabel('EC [mS/m]')\n",
    "axr.set_ylabel('depth [m]')\n",
    "axr.plot((profile_data[dataset_name]),profile_data['z'], label='original (DC) ERT EC',)\n",
    "if not config['n_int']: \n",
    "    axr.plot(con[:-1]*1000,-thick, '.', label='Model EC 9khz',color = 'red')\n",
    "else:\n",
    "    axr.plot(con*1000,-thick, '.', label='Model EC 9khz',color = 'red')\n",
    "axr.set_title(f'Reference profile: ID {profile_id}')\n",
    "\n",
    "conductivities = con*1000\n",
    "print('conductivities', conductivities)\n",
    "\n",
    "ec_cols_ref = []\n",
    "if 'end' in config['interface']:\n",
    "    config['interface'].remove('end')\n",
    "# Get conductivity stats for bounds\n",
    "if config['n_int']:\n",
    "    if 'end' in ec_cols_ref:\n",
    "        ec_cols_ref.remove('end')\n",
    "    ec_cols_ref = config['interface']\n",
    "    ec_cols_ref.append('end')\n",
    "    mod_layers = thick[1:]\n",
    "else:\n",
    "    if len(conductivities) == len(thick):\n",
    "        mod_layers = thick[1:]\n",
    "        print(f\"length modlayers = {len(mod_layers)} with {len(conductivities)} conductivities\")\n",
    "    elif len(conductivities) == (len(thick)+1):\n",
    "        mod_layers = thick\n",
    "        print(f\"length modlayers = {len(mod_layers)} with {len(conductivities)} conductivities\")\n",
    "    else:\n",
    "        raise ValueError(f\"Check length of conductivities ({len(conductivities)}) and layers ({len(thick)}) arrays!!\")\n",
    "    \n",
    "    ec_cols_ref = np.round(layers_interfaces,decimals=1).tolist()\n",
    "ec_df = pd.DataFrame(columns=ec_cols_ref)\n",
    "\n",
    "# \n",
    "for i in merged_df['ID'].unique(): \n",
    "    profile_data = merged_df[merged_df['ID'] == i].copy()\n",
    "    if not config['n_int']:\n",
    "        if abs(profile_data.iloc[0]['z']) > max((list(map(abs, ec_cols_ref)))):\n",
    "            #print(f'removed {profile_data.iloc[0][\"z\"]}')\n",
    "            profile_data = profile_data.drop(profile_data.iloc[0].name)\n",
    "        elif abs(profile_data.iloc[-1]['z']) < 0.1:\n",
    "            #print(f'removed {profile_data.iloc[-1][\"z\"]}')\n",
    "            profile_data = profile_data.drop(profile_data.iloc[-1].name)\n",
    "    res_col = 'EC(mS/m)'\n",
    "    depth = 'z' \n",
    "    con_m = profile_data[res_col].values\n",
    "    layers_interfaces = np.cumsum(models[i].thick)\n",
    "    layers_interfaces = np.insert(layers_interfaces, 0, 0)\n",
    "    num_layers = len(con)\n",
    "    perm = np.full(num_layers, constants.epsilon_0)\n",
    "    sus = np.zeros(num_layers)\n",
    "\n",
    "    first_lay = profile_data[depth].iloc[-1].round(decimals=1)\n",
    "    second_lay = profile_data[depth].iloc[-2].round(decimals=1)\n",
    "\n",
    "    if not config['n_int']:\n",
    "        first_lay = profile_data[depth].iloc[-1].round(decimals=1)\n",
    "        second_lay = profile_data[depth].iloc[-2].round(decimals=1)\n",
    "        if first_lay == 0:\n",
    "            profile_data[depth]=profile_data[depth] +second_lay\n",
    "        else:\n",
    "            profile_data[depth]=profile_data[depth] +first_lay\n",
    "        thick = -profile_data[depth].iloc[1:].values\n",
    "    else:\n",
    "        thick = -profile_data[depth].values\n",
    "\n",
    "    ec_df = pd.concat([ec_df, pd.DataFrame([np.flip(con_m)], columns=ec_cols_ref)])\n",
    "\n",
    "ec_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "ec_stats = ec_df.describe().loc[['min', 'max', 'std', '50%', 'mean']]\n",
    "ec_stats.rename(index={'50%': 'median'}, inplace=True)\n",
    "ec_stats.loc['min_sd'] = ec_stats.loc['min'] - 2 * ec_stats.loc['std']\n",
    "ec_stats.loc['max_sd'] = ec_stats.loc['max'] + 2 * ec_stats.loc['std']\n",
    "\n",
    "position = -thick\n",
    "\n",
    "\n",
    "# define parameters for inversion starting model\n",
    "# --------------------------------------------- #\n",
    "\n",
    "if not config['n_int']:\n",
    "    minstat = np.flipud(ec_stats.loc['min'].values[1:])\n",
    "    maxstat = np.flipud(ec_stats.loc['max'].values[1:])\n",
    "    start_mod = ec_stats.loc['mean'].values[1:]\n",
    "    boundcols = ec_cols_ref[:-1]\n",
    "else:\n",
    "    minstat = np.flipud(ec_stats.loc['min'].values)\n",
    "    maxstat = np.flipud(ec_stats.loc['max'].values)\n",
    "    start_mod = ec_stats.loc['mean'].values\n",
    "\n",
    "axr.plot(np.flipud(start_mod),position, \n",
    "            '*', \n",
    "            label='average conductivity',\n",
    "            color = 'green',\n",
    "            alpha = 0.5)\n",
    "axr.plot(minstat,position, \n",
    "            '.', \n",
    "            label='min',\n",
    "            color = 'black',\n",
    "            alpha = 0.2)\n",
    "axr.plot(maxstat,position, \n",
    "            '+', \n",
    "            label='max',\n",
    "            color = 'black',\n",
    "            alpha = 0.25)\n",
    "\n",
    "axr.legend()\n",
    "if config['constrain']:\n",
    "    if config['custom_bounds']:\n",
    "        bounds = config['bounds']\n",
    "    else:\n",
    "        bounds = []\n",
    "        for i, name in enumerate(ec_cols_ref):\n",
    "            if ec_stats.loc['min_sd'][name] > 0:\n",
    "                min = ec_stats.loc['min_sd'][name]\n",
    "            elif ec_stats.loc['min'][name] > 0:\n",
    "                min = ec_stats.loc['min'][name]\n",
    "            else:\n",
    "                min = 10\n",
    "            max = ec_stats.loc['max_sd'][name]\n",
    "            min_max = tuple([min,max])\n",
    "            bounds.append(min_max)\n",
    "        bounds = np.round(bounds, decimals=0)\n",
    "        if not config['n_int'] and not config['custom_bounds']:\n",
    "            bounds = bounds[1:]\n",
    "        print(f'autobounds = {bounds}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform inversion on sampling locations (to be used in pedophysical modelling)\n",
    "\n",
    "if 'code' in em_samples.columns:\n",
    "    em_samples = em_samples.rename(columns={'code': 'ID'})\n",
    "\n",
    "i = instrument.niter\n",
    "n = 4\n",
    "em_samples.columns.values[n:n+i]\n",
    "\n",
    "if config['instrument_code'] == 'Dualem-21HS':\n",
    "    new_columns = emp_21HS\n",
    "else:\n",
    "    new_columns = emp_421S\n",
    "\n",
    "if len(new_columns) != i:\n",
    "    raise ValueError(\"The length of new_columns must be equal to the number of columns to rename\")\n",
    "else:\n",
    "    em_samples.columns.values[n:n+i] = new_columns\n",
    "\n",
    "em_samples.to_csv(temp_file)\n",
    "\n",
    "# transect inversion settings\n",
    "\n",
    "s_rec = Problem()\n",
    "s_rec.createSurvey(temp_file)\n",
    "#t_rec.rollingMean(window=12)\n",
    "\n",
    "s_rec.setInit(\n",
    "    depths0=np.flipud(mod_layers),\n",
    "    conds0=conductivities\n",
    "    )\n",
    "\n",
    "if config['remove_coil']:\n",
    "    if type(config['coil_n']) == list:\n",
    "        config['coil_n'] = sorted(config['coil_n'])\n",
    "        for i in enumerate(config['coil_n']):\n",
    "            r_coil = s_rec.coils[(config['coil_n'][i[0]]-i[0])]\n",
    "            # print(f'removing {r_coil}')\n",
    "            s_rec.removeCoil(config['coil_n'][i[0]]-i[0])\n",
    "    else:\n",
    "        s_rec.removeCoil(config['coil_n'])\n",
    "\n",
    "print(f'Data used for inversion: {s_rec.coils}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invert using ROPE solver (RObust Parameter Estimation)\n",
    "warnings.filterwarnings('ignore')\n",
    "opt_meth = config['opt_method']\n",
    "inv_meth = config['fs_emp']\n",
    "reg_meth = config['regularization']\n",
    "alph_param = config['alpha']\n",
    "if opt_meth in ['MCMC', 'ROPE']:\n",
    "    if config['constrain']:\n",
    "        \n",
    "        print(f'Constrained inversion using {inv_meth} with {opt_meth}, reg={reg_meth}, alpha={alph_param}')\n",
    "        s_rec.invert(forwardModel=config['fs_emp'], method=opt_meth, \n",
    "                regularization=reg_meth, alpha=alph_param, \n",
    "                bnds=bounds\n",
    "                )\n",
    "\n",
    "    else:\n",
    "        print(f'Inversion using {inv_meth} with {opt_meth}, reg={reg_meth}, alpha={alph_param}')\n",
    "        s_rec.invert(forwardModel=config['fs_emp'], method=opt_meth, \n",
    "        regularization=reg_meth, alpha=alph_param, njobs=-1\n",
    "        )\n",
    "\n",
    "else:\n",
    "    print(f'Inversion using {inv_meth} with {opt_meth}, reg={reg_meth}, alpha={alph_param}')\n",
    "    s_rec.invert(forwardModel='FSeq', method='Gauss-Newton', alpha=alph_param,regularization=reg_meth)\n",
    "s_rec.showOne2one()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1: Plot the inversion results and put outcomes into a pandas dataframe\n",
    "# ------------------------------------------------------------------------\n",
    "csv_filename = f'{now}_{emfile_prefix}_inverted_samples_{opt_meth}_04.csv'\n",
    "\n",
    "# ******************************************************************** #\n",
    "\n",
    "# Plot inversion outcomes down to a max depth of 2 m, and plotting the data\n",
    "# based on their true coordinates along the transect (dist=True).\n",
    "s_rec.showResults(dist=True, errorbar = True) \n",
    "\n",
    "# Extracting the values from the first row of the transect.depths[0] array\n",
    "depth_values = s_rec.depths[0][0]\n",
    "\n",
    "# Creating the custom column names for layer_cols\n",
    "layer_cols = ['EC_{:.2f}'.format(d) for d in depth_values] + ['EC_end']\n",
    "\n",
    "# Combining the data from the 'x', 'y' columns and the transect.models[0] array\n",
    "data = np.c_[s_rec.surveys[0].df[['x', 'y']].values, s_rec.models[0]]\n",
    "\n",
    "# Creating the final dataframe with the desired column names\n",
    "ds_inv = pd.DataFrame(data, columns=['x', 'y'] + layer_cols)\n",
    "ds_inv['pos'] = em_samples['ID'].to_numpy()\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "# Export the dataframe as a csv-file\n",
    "outfile_transect = os.path.join(inv_folder, csv_filename)\n",
    "ds_inv.to_csv(outfile_transect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_columns = ds_inv.columns[3:-1]\n",
    "ds_c[inv_columns] = np.nan\n",
    "\n",
    "for idc, c in enumerate(inv_columns):\n",
    "\n",
    "    for i in range(len(ds_inv.x)):\n",
    "        ds_c.loc[ds_c.code == i+1, c] = ds_inv.loc[i, c]\n",
    "\n",
    "def closest_ec(row):\n",
    "    depth = row['depth']\n",
    "    # Filter columns that start with 'EC_' but not 'EC_end'\n",
    "    ec_cols = [col for col in row.index if col.startswith('EC_') and col != 'EC_end']\n",
    "    # Convert the part after 'EC_' to float and calculate the absolute difference with depth\n",
    "    differences = {col: abs(depth/100 - float(col.split('_')[1])) for col in ec_cols}\n",
    "    # Find the column name with the minimum difference\n",
    "    closest_col = min(differences, key=differences.get)\n",
    "    return row[closest_col]\n",
    "\n",
    "# Apply the function to each row\n",
    "ds_c['bulk_ec_inv'] = ds_c.apply(closest_ec, axis=1)\n",
    "\n",
    "#Obtain EC DC TC\n",
    "ds_c['bulk_ec_dc_tc_inv'] = predict.BulkECDCTC(Soil(temperature = ds_c.temp.values+273.15,\n",
    "                                                    frequency_ec = 9e3,\n",
    "                                                    bulk_ec = ds_c.bulk_ec_inv.values/1000))\n",
    "# Mean of input inverted EC DC TC values\n",
    "EC_mean = np.mean(ds_c['bulk_ec_dc_tc_inv'].values) \n",
    "print('EC_mean', EC_mean)\n",
    "\n",
    "#### Uncertainty inversion parameters\n",
    "EC = 0.06980570031133289\n",
    "\n",
    "### ROPE uncertainty\n",
    "inv_results = [0.075, 0.069, 0.073, 0.0687, 0.0695, 0.0719, 0.0745, 0.0709, 0.0644, 0.0708]\n",
    "\n",
    "ROPE_inv_upper_p = 100*(-EC + np.max(inv_results))/(np.max(inv_results))\n",
    "ROPE_inv_lower_p = 100*(-EC + np.min(inv_results))/(np.min(inv_results))\n",
    "print('ROPE_inv_upper_p, ROPE_inv_lower_p', ROPE_inv_upper_p, ROPE_inv_lower_p)\n",
    "\n",
    "### Alpha uncertainty\n",
    "Alpha_upper = np.inf\n",
    "Alpha_lower = 0.001\n",
    "Alpha = 0.07\n",
    "\n",
    "Alpha_upper_p = 100*(Alpha_upper - Alpha)/Alpha\n",
    "Alpha_lower_p = 100*(-Alpha + Alpha_lower)/Alpha\n",
    "print('Alpha_upper_p, Alpha_lower_p', Alpha_upper_p, Alpha_lower_p)\n",
    "\n",
    "sens_alpha_upper = EC_upper_p/Alpha_upper_p\n",
    "sens_alpha_lower = EC_lower_p/Alpha_lower_p\n",
    "print('sens_alpha_upper, sens_alpha_lower', sens_alpha_upper, sens_alpha_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.write('\\t\"EC_mean\":\"{}\",'.format(EC_mean) + '\\n')\n",
    "file.write('\\t\"ROPE_inv_upper_p\":\"{}\",'.format(ROPE_inv_upper_p) + '\\n')\n",
    "file.write('\\t\"ROPE_inv_lower_p\":\"{}\",'.format(ROPE_inv_lower_p) + '\\n')\n",
    "\n",
    "file.write('\\t\"input file + path\": \"{}\",'.format(infile) + '\\n\\n')\n",
    "file.write('\\t\"instrument\": \"{}\",'.format(config['instrument_code'] ) + '\\n')\n",
    "file.write('\\t\"instrument mode\": \"{}\",'.format(config['instrument_orientation']) + '\\n')\n",
    "file.write('\\t\"instrument height (m)\": {:.3f},'.format(config['instrument_height']) + '\\n')\n",
    "\n",
    "if config['remove_coil']:\n",
    "    rem_coils = instrument.cc_names[config['coil_n']]\n",
    "    file.write('\\t\"configurations not used in inversion\": \"{}\",'.format(rem_coils) + '\\n\\n')\n",
    "\n",
    "file.write('\\t\"forward model\": \"{}\",'.format(config['fs_emp']) + '\\n')\n",
    "file.write('\\t\"optimisation method\":\"{}\",'.format(config['opt_method']) + '\\n')\n",
    "file.write('\\t\"regularisation\": \"{}\",'.format(config['regularization']) + '\\n')\n",
    "file.write('\\t\"alpha parameter\": \"{}\",'.format(alph_param) + '\\n\\n')\n",
    "file.write('\\t\"reference EC profile\":\"{}\",'.format(config['reference_profile']) + '\\n')\n",
    "\n",
    "if config['constrain']:\n",
    "    file.write('\\t \"constrained inversion\":' + '\\n')\n",
    "    if config['n_int']:\n",
    "        file.write('\\t\"custom interface boundaries\": \"{}\"\\n'.format(config['interface']) + '\\n')\n",
    "    if config['custom_bounds']:\n",
    "        file.write('\\t\"custom inversion constraints (bnds)\": \"{}\" \\n'.format(config['bounds']) + '\\n')\n",
    "    else:\n",
    "        file.write('\\t\"automated inversion constraints (bnds)\": \"{}\"\\n'.format(bounds) + '\\n')\n",
    "\n",
    "file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
