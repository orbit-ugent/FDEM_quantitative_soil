{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uncertainty analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SA_all import SA\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from plots import *\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Define the variables and their possible values\n",
    "site = 'M'\n",
    "\n",
    "clls = [0.2, 0.3]\n",
    "percents = [10, 20, 30]\n",
    "sample_locs = ['mean', 'closest']\n",
    "interfaces = ['observed', 'log-defined']\n",
    "FMs = ['FSeq', 'CS', 'FSlin']\n",
    "MinMs = ['Gauss-Newton', 'ROPE']\n",
    "alphas = [0.01, 0.07, 0.2]\n",
    "remove_coils = [True, False]\n",
    "start_avgs = [True, False]\n",
    "constrains = [True, False]\n",
    "\n",
    "SA_results = 'SA_results/'\n",
    "\n",
    "file_path_all = 'dt'+site+'_'+str(clls)+'_'+str(percents)+'_'+str(sample_locs)+'_'+str(['obs', 'logd'])+'_'+str(FMs)+'_'+str(['GN', 'ROPE'])+'_'+str(alphas)+'_'+str(['T', 'F'])+'_'+str(['T', 'F'])+'_'+str(['T', 'F'])+'_det'\n",
    "\n",
    "# Initialize DataFrame\n",
    "dt = pd.DataFrame()\n",
    "i = 0  # to keep track of iterations for saving purposes\n",
    "exist_fine = 0 \n",
    "exist_failed = 0\n",
    "rescatados = 0\n",
    "still_failed = 0\n",
    "\n",
    "# Iterate over all combinations\n",
    "for combination in itertools.product(clls, percents, sample_locs, interfaces, FMs, MinMs, alphas, remove_coils, start_avgs, constrains):\n",
    "\n",
    "    cl, percent, sample_loc, interface, FM, MinM, alpha, remove_coil, start_avg, constrain = combination\n",
    "    print('Iteration:', i, 'Combination:', combination)\n",
    "\n",
    "    file_path = 'dt'+site+'_'+str(cl)+'_'+str(percent)+'_'+str(sample_loc)+'_'+str(interface)+'_'+str(FM)+'_'+str(MinM)+'_'+str(alpha)+'_'+str(remove_coil)+'_'+str(start_avg)+'_'+str(constrain)+'_det'\n",
    "    SA_file_path = SA_results + file_path+'.csv'\n",
    "    file_path_FAILED = SA_results + file_path+'_FAILED'+'.csv'\n",
    "\n",
    "    if os.path.exists(SA_file_path):\n",
    "        #print('exists')\n",
    "        SA_file = pd.read_csv(SA_file_path)\n",
    "        exist_fine += 1\n",
    "\n",
    "    elif os.path.exists(file_path_FAILED):\n",
    "        #print('exists but failed')\n",
    "        exist_failed += 1\n",
    "\n",
    "        try:\n",
    "            # Call the SA function with the current combination\n",
    "            results = SA(site, cl, percent, sample_loc, interface, FM, MinM, alpha, remove_coil, start_avg, constrain)\n",
    "\n",
    "            # Create a dictionary for the current iteration's results\n",
    "            current_results = [{'cl': cl, 'percent': percent, 'Samples location': sample_loc, 'Interface': interface, 'Forward_Model': FM, 'Minimization_Method': MinM, 'Alpha': alpha,\n",
    "                                'remove_coil': remove_coil, 'start_avg': start_avg, 'constrain': constrain,\n",
    "                                'Det': det, 'R2': res[0], 'RMSE': res[1], '0R2': res[2], '0RMSE': res[3]}\n",
    "                            for det, res in zip(['LT', 'ID', 'LS'], [(results[i], results[i+1], results[i+10], results[i+11]) for i in range(0, 6, 2)])]\n",
    "\n",
    "            print('current_results', current_results)\n",
    "            SA_file = pd.DataFrame(current_results)\n",
    "            # Save the DataFrame to CSV after each iteration\n",
    "            SA_file.to_csv(SA_file_path, index=False)\n",
    "            os.remove(file_path_FAILED)\n",
    "            rescatados += 1\n",
    "\n",
    "        except Exception as e:\n",
    "\n",
    "            # Create a dictionary for the current iteration's results\n",
    "            current_results = [{'cl': cl, 'percent': percent, 'Samples location': sample_loc, 'Interface': interface, 'Forward_Model': FM, 'Minimization_Method': MinM, 'Alpha': alpha,\n",
    "                                'remove_coil': remove_coil, 'start_avg': start_avg, 'constrain': constrain,\n",
    "                                'Det': det, 'R2': np.nan, 'RMSE': np.nan, '0R2': np.nan, '0RMSE': np.nan}\n",
    "                            for det, res in zip(['LT', 'ID', 'LS'], [(np.nan, np.nan, np.nan, np.nan) for i in range(0, 6, 2)])]\n",
    "            \n",
    "            SA_file_FAILED = pd.DataFrame(current_results)\n",
    "            #print(f\"An error occurred: {e} with combination {combination}\")\n",
    "            SA_file_FAILED.to_csv(file_path_FAILED, index=False)\n",
    "            still_failed += 1\n",
    "\n",
    "            continue  # Continue to the next iteration even if an error occurs\n",
    "            \n",
    "\n",
    "    #print('SA_file.head()', SA_file.head())\n",
    "    #print('dt', dt.head())\n",
    "    # Append current results to the DataFrame\n",
    "    dt = pd.concat([dt, SA_file])\n",
    "    i += 1  # Increment the iteration counter\n",
    "\n",
    "print('exist_fine', exist_fine)\n",
    "print('exist_failed', exist_failed)\n",
    "print('rescatados', rescatados)\n",
    "print('still_failed', still_failed)\n",
    "\n",
    "print(dt)\n",
    "dt.to_csv(SA_results + file_path_all+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import numpy as np\n",
    "from plots import *\n",
    "from plots import *\n",
    "indicator = 'RMSE'\n",
    "\n",
    "clls = [0.2, 0.3]\n",
    "percents = [10, 20, 30]\n",
    "sample_locs = ['mean', 'closest']\n",
    "interfaces = ['observed', 'log-defined']\n",
    "FMs = ['FSeq', 'CS', 'FSlin']\n",
    "MinMs = ['Gauss-Newton', 'ROPE']\n",
    "alphas = [0.01, 0.07, 0.2]\n",
    "remove_coils = [True, False]\n",
    "start_avgs = [True, False]\n",
    "constrains = [True, False]\n",
    "\n",
    "SA_results = 'SA_results/'\n",
    "\n",
    "# Example usage\n",
    "file_path_all_ = str(clls)+'_'+str(percents)+'_'+str(sample_locs)+'_'+str(['obs', 'logd'])+'_'+str(FMs)+'_'+str(['GN', 'ROPE'])+'_'+str(alphas)+'_'+str(['T', 'F'])+'_'+str(['T', 'F'])+'_'+str(['T', 'F'])+'_det'\n",
    "\n",
    "SA_plot(file_path_all_, SA_results, 'RMSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from plots import *\n",
    "from plots import *\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Define the variables and their possible values\n",
    "site = 'M'\n",
    "\n",
    "clls = [0.2, 0.3, 0.4]\n",
    "percents = [10, 20, 30]\n",
    "sample_locs = ['mean', 'closest']\n",
    "#interfaces = ['observed', 'log-defined']\n",
    "FMs = ['FSeq', 'CS', 'FSlin']\n",
    "MinMs = ['Gauss-Newton', 'ROPE']\n",
    "alphas = [0.01, 0.07, 0.2]\n",
    "remove_coils = [True, False]\n",
    "start_avgs = [True, False]\n",
    "constrains = [True, False]\n",
    "\n",
    "SA_results = 'SA_results/'\n",
    "\n",
    "i = 0  # to keep track of iterations for saving purposes\n",
    "# Iterate over all combinations\n",
    "for combination in itertools.product(clls, percents, sample_locs, FMs, MinMs, alphas, remove_coils, start_avgs, constrains):\n",
    "\n",
    "    cl, percent, sample_loc, FM, MinM, alpha, remove_coil, start_avg, constrain = combination\n",
    "    print('Iteration:', i, 'Combination:', combination)\n",
    "\n",
    "    file_path= 'dt'+site+'_'+str(cl)+'_'+str(percent)+'_'+str(sample_loc)+'_Observed'+'_'+str(FM)+'_'+str(MinM)+'_'+str(alpha)+'_'+str(remove_coil)+'_'+str(start_avg)+'_'+str(constrain)+'_det'    \n",
    "    file_path_new = 'dt'+site+'_'+str(cl)+'_'+str(percent)+'_'+str(sample_loc)+'_observed'+'_'+str(FM)+'_'+str(MinM)+'_'+str(alpha)+'_'+str(remove_coil)+'_'+str(start_avg)+'_'+str(constrain)+'_det'    \n",
    "    SA_file_path = SA_results + file_path+'.csv'\n",
    "    file_path_FAILED = SA_results + file_path+'_FAILED'+'.csv'\n",
    "\n",
    "    SA_file_path_new = SA_results + file_path_new+'.csv'\n",
    "    file_path_FAILED_new = SA_results + file_path_new+'_FAILED'+'.csv'\n",
    "\n",
    "    if os.path.exists(SA_file_path):\n",
    "        SA_file = pd.read_csv(SA_file_path)\n",
    "        SA_file.to_csv(SA_file_path_new, index=False)\n",
    "\n",
    "    elif os.path.exists(file_path_FAILED):\n",
    "        SA_file_FAILED = pd.read_csv(file_path_FAILED)\n",
    "        SA_file_FAILED.to_csv(file_path_FAILED_new, index=False)\n",
    "\n",
    "    i += 1  # Increment the iteration counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from plots import *\n",
    "from plots import *\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Define the variables and their possible values\n",
    "site = 'M'\n",
    "\n",
    "clls = [0.2, 0.3, 0.4]\n",
    "percents = [10, 20, 30]\n",
    "sample_locs = ['mean', 'closest']\n",
    "#interfaces = ['Observed', 'Log-defined']\n",
    "FMs = ['FSeq', 'CS', 'FSlin']\n",
    "MinMs = ['Gauss-Newton', 'ROPE']\n",
    "alphas = [0.01, 0.07, 0.2]\n",
    "remove_coils = [True, False]\n",
    "start_avgs = [True, False]\n",
    "constrains = [True, False]\n",
    "\n",
    "data_inv_folder = 'data/inverted/'\n",
    "\n",
    "i = 0  # to keep track of iterations for saving purposes\n",
    "# Iterate over all combinations\n",
    "for combination in itertools.product(clls, percents, sample_locs, FMs, MinMs, alphas, remove_coils, start_avgs, constrains):\n",
    "\n",
    "    cl, percent, sample_loc, FM, MinM, alpha, remove_coil, start_avg, constrain = combination\n",
    "    print('Iteration:', i, 'Combination:', combination)\n",
    "\n",
    "    file_path = 'proefhoeve_21HS_inverted_samples'+'_'+str(cl)+'_'+str(percent)+'_'+str(sample_loc)+'_'+str(FM)+'_'+str(MinM)+'_'+str(alpha)+'_'+str(remove_coil)+'_'+str(start_avg)+'_'+str(constrain)\n",
    "    file_path_new = 'proefhoeve_21HS_inverted_samples'+'_'+str(cl)+'_'+str(percent)+'_'+str(sample_loc)+'_Observed'+'_'+str(FM)+'_'+str(MinM)+'_'+str(alpha)+'_'+str(remove_coil)+'_'+str(start_avg)+'_'+str(constrain)\n",
    "\n",
    "    data_inv_path = data_inv_folder + file_path+'.csv'\n",
    "    data_inv_path_new = data_inv_folder + file_path_new+'.csv'\n",
    "\n",
    "    #file_path_FAILED = SA_results + file_path+'_FAILED'+'.csv'\n",
    "\n",
    "    #SA_file_path_new = SA_results + file_path_new+'.csv'\n",
    "    #file_path_FAILED_new = SA_results + file_path_new+'_FAILED'+'.csv'\n",
    "\n",
    "    if os.path.exists(data_inv_path):\n",
    "        inv_file = pd.read_csv(data_inv_path)\n",
    "        inv_file.to_csv(data_inv_path_new, index=False)\n",
    "        i += 1\n",
    "#    elif os.path.exists(file_path_FAILED):\n",
    "#        SA_file_FAILED = pd.read_csv(file_path_FAILED)\n",
    "#        cols = list(SA_file_FAILED.columns)\n",
    "#        cols.remove('Interface')\n",
    "#        cols.insert(3, 'Interface')\n",
    "#        SA_file_FAILED = SA_file_FAILED[cols]\n",
    "#        SA_file_FAILED['Interface'] = 'Observed'\n",
    "#        SA_file_FAILED.to_csv(file_path_FAILED, index=False)\n",
    "\n",
    "#    i += 1  # Increment the iteration counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "# Packages -----------------\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import warnings\n",
    "import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy import constants\n",
    "from matplotlib.path import Path\n",
    "\n",
    "# Get notebook and parent dir\n",
    "current_dir = os.path.dirname(os.path.abspath('__file__')) \n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "\n",
    "# Set path to pedophysics module \n",
    "pedophysics_code_path = os.path.join(parent_dir)\n",
    "sys.path.insert(0, pedophysics_code_path)\n",
    "\n",
    "import pedophysics\n",
    "from pedophysics import predict, Soil\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.optimize import root\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.optimize import minimize, differential_evolution\n",
    "from IPython.display import clear_output\n",
    "from utils.spatial_utils import utm_to_epsg, get_coincident\n",
    "#!pip install pymel\n",
    "import pymel\n",
    "from FDEM import Initialize\n",
    "from utils.profile_utils import merge_layers, plot_profile, check_uniformity_and_interpolate\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import pearsonr\n",
    "from PyPDF2 import PdfMerger\n",
    "from emagpy import Problem\n",
    "\n",
    "# Electromagnetic induction data inversion package\n",
    "from plots import *\n",
    "from PM import *\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.options.future.infer_string = True\n",
    "\n",
    "sys.path.insert(0,'../src/') # this add the emagpy/src directory to the PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pedophysical modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datetime for filename\n",
    "now = (datetime.datetime.now())\n",
    "now = now.strftime(\"%y%m%d_%H%M\")\n",
    "\n",
    "# User input\n",
    "s_site = 'M'; # P = Proefhoeve; M = Middelkerke\n",
    "# Define input datatype and source folder\n",
    "em_intype = 'rec'   # 'rec' = rECa transect; 'lin' = LIN ECa transect; \n",
    "                    # 'survey' = rEC full survey\n",
    "\n",
    "config = {}\n",
    "\n",
    "config['instrument_code'] = 'Dualem-21HS' # instrument code\n",
    "\n",
    "cal = 'calibrated' # 'non_calibrated', 'drift_calibrated'\n",
    "instrument_code = '21HS' # 421S, '21HS'\n",
    "\n",
    "# User input\n",
    "\n",
    "datafolder = 'data' # data folder\n",
    "\n",
    "if s_site == 'P':\n",
    "    profile_prefix = 'proefhoeve'\n",
    "    if config['instrument_code'] == 'Dualem-21HS':\n",
    "        emfile_prefix = 'proefhoeve_21HS'\n",
    "    else: \n",
    "        emfile_prefix = 'proefhoeve_421S'\n",
    "        \n",
    "else:\n",
    "    profile_prefix = 'middelkerke'\n",
    "    emfile_prefix = 'middelkerke_421S'\n",
    "    # check if correct instrument (only 421S data available for Middelkerke)\n",
    "    if config['instrument_code'] == 'Dualem-21HS':\n",
    "        config['instrument_code'] = 'Dualem-421S'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data import dry down experiment\n",
    "dry_down = os.path.join(datafolder, f'Dry_down.csv')\n",
    "dry_d = pd.read_csv(dry_down, sep=',', header=0)\n",
    "\n",
    "cal_folder = os.path.join(datafolder, 'calibrated')\n",
    "em_survey = os.path.join(cal_folder, f'{emfile_prefix}_calibrated_rECa.csv')\n",
    "em_survey = pd.read_csv(em_survey, sep=',', header=0)\n",
    "sampleprop = os.path.join(datafolder, f'{profile_prefix}_soil_analysis.csv')\n",
    "samples_analysis = pd.read_csv(sampleprop, sep=',', header=0)\n",
    "\n",
    "em_sample_prop = get_coincident(em_survey, samples_analysis)\n",
    "ds_c = em_sample_prop.copy()\n",
    "\n",
    "inverted_data = False # Include inverted data or not\n",
    "\n",
    "if inverted_data:\n",
    "    inverted = os.path.join(datafolder, f'{profile_prefix}_inverted_samples_{instrument_code}c.csv')\n",
    "    ds_inv = pd.read_csv(inverted, sep=',', header=0)\n",
    "    print(ds_inv.head())\n",
    "\n",
    "    inv_columns = ds_inv.columns[3:-1]\n",
    "    ds_c[inv_columns] = np.nan\n",
    "\n",
    "    for idc, c in enumerate(inv_columns):\n",
    "\n",
    "        for i in range(len(ds_inv.x)):\n",
    "            ds_c.loc[ds_c.code == i+1, c] = ds_inv.loc[i, c]\n",
    "\n",
    "    def closest_ec(row):\n",
    "        depth = row['depth']\n",
    "        # Filter columns that start with 'EC_' but not 'EC_end'\n",
    "        ec_cols = [col for col in row.index if col.startswith('EC_') and col != 'EC_end']\n",
    "        # Convert the part after 'EC_' to float and calculate the absolute difference with depth\n",
    "        differences = {col: abs(depth/100 - float(col.split('_')[1])) for col in ec_cols}\n",
    "        # Find the column name with the minimum difference\n",
    "        closest_col = min(differences, key=differences.get)\n",
    "        return row[closest_col]\n",
    "\n",
    "\n",
    "    # Apply the function to each row\n",
    "    ds_c['bulk_ec_inv'] = ds_c.apply(closest_ec, axis=1)\n",
    "\n",
    "    #Obtain EC DC TC\n",
    "    ds_c['bulk_ec_dc_tc_inv'] = predict.BulkECDCTC(Soil(temperature = ds_c.temp.values+273.15,\n",
    "                                                        frequency_ec = 9e3,\n",
    "                                                        bulk_ec = ds_c.bulk_ec_inv.values/1000))\n",
    "    # Mean of input inverted EC DC TC values\n",
    "    EC_mean = np.mean(ds_c['bulk_ec_dc_tc_inv'].values) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deterministic modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caclculate Bulk EC from HydraProbe data at 50Mhz\n",
    "offset = 4\n",
    "water_perm = 80\n",
    "ds_c['bulk_ec_hp'] = logsdon(50e6, ds_c.rperm, ds_c.iperm)\n",
    "\n",
    "ds_c['bulk_ec_dc_hp'] = predict.BulkECDC(Soil(frequency_ec = 50e6,\n",
    "                                              bulk_ec = ds_c.bulk_ec_hp.values))\n",
    "\n",
    "ds_c['bulk_ec_tc_hp'] = SheetsHendrickxEC( ds_c.bulk_ec_hp, ds_c.temp)\n",
    "ds_c['bulk_ec_dc_tc_hp'] = predict.BulkECDCTC(Soil(temperature = ds_c.temp.values,\n",
    "                                                    bulk_ec_dc = ds_c.bulk_ec_dc_hp.values\n",
    "                                                    ))\n",
    "\n",
    "# Caclculate Water EC from HydraProbe data at 50Mhz\n",
    "ds_c['water_ec_hp'] = Hilhorst(ds_c.bulk_ec_hp, ds_c.rperm, water_perm, offset)\n",
    "ds_c['water_ec_hp_t'] = WraithOr(ds_c.water_ec_hp, ds_c.temp)\n",
    "ds_c['iperm_water_t'] = ds_c.water_ec_hp_t/(50e6*2*pi*epsilon_0)\n",
    "\n",
    "# -------------------------------------------------------------------------------------\n",
    "\n",
    "# DRY DOWN experiment\n",
    "dry_d['P_top_EC'] = logsdon(50e6, dry_d.P_top_RP, dry_d.P_top_IP)\n",
    "dry_d['P_bot_EC'] = logsdon(50e6, dry_d.P_bot_RP, dry_d.P_bot_IP)\n",
    "dry_d['M_top_EC'] = logsdon(50e6, dry_d.M_top_RP, dry_d.M_top_IP)\n",
    "dry_d['M_bot_EC'] = logsdon(50e6, dry_d.M_bot_RP, dry_d.M_bot_IP)\n",
    "\n",
    "dry_d['P_top_ECw'] = predict.WaterEC(Soil(bulk_ec_dc = dry_d.P_top_EC.values,\n",
    "                                          water = dry_d.P_top_W.values,\n",
    "                                          temperature = dry_d.P_top_T.values+273.15))\n",
    "\n",
    "dry_d['P_bot_ECw'] = predict.WaterEC(Soil(bulk_ec_dc = dry_d.P_bot_EC.values,\n",
    "                                          water = dry_d.P_bot_W.values,\n",
    "                                          temperature = dry_d.P_bot_T.values+273.15))\n",
    "\n",
    "dry_d['M_top_ECw'] = predict.WaterEC(Soil(bulk_ec_dc = dry_d.M_top_EC.values,\n",
    "                                          water = dry_d.M_top_W.values,\n",
    "                                          temperature = dry_d.M_top_T.values+273.15))\n",
    "\n",
    "dry_d['M_bot_ECw'] = predict.WaterEC(Soil(bulk_ec_dc = dry_d.M_bot_EC.values,\n",
    "                                          water = dry_d.M_bot_W.values,\n",
    "                                          temperature = dry_d.M_bot_T.values+273.15))\n",
    "\n",
    "# DRY DOWN experiment\n",
    "dry_d['P_top_EC'] = logsdon(50e6, dry_d.P_top_RP, dry_d.P_top_IP)\n",
    "dry_d['P_bot_EC'] = logsdon(50e6, dry_d.P_bot_RP, dry_d.P_bot_IP)\n",
    "dry_d['M_top_EC'] = logsdon(50e6, dry_d.M_top_RP, dry_d.M_top_IP)\n",
    "dry_d['M_bot_EC'] = logsdon(50e6, dry_d.M_bot_RP, dry_d.M_bot_IP)\n",
    "\n",
    "dry_d['P_top_ECw'] = predict.WaterEC(Soil(bulk_ec_dc = dry_d.P_top_EC.values,\n",
    "                                          water = dry_d.P_top_W.values,\n",
    "                                          temperature = dry_d.P_top_T.values+273.15))\n",
    "\n",
    "dry_d['P_bot_ECw'] = predict.WaterEC(Soil(bulk_ec_dc = dry_d.P_bot_EC.values,\n",
    "                                          water = dry_d.P_bot_W.values,\n",
    "                                          temperature = dry_d.P_bot_T.values+273.15))\n",
    "\n",
    "dry_d['M_top_ECw'] = predict.WaterEC(Soil(bulk_ec_dc = dry_d.M_top_EC.values,\n",
    "                                          water = dry_d.M_top_W.values,\n",
    "                                          temperature = dry_d.M_top_T.values+273.15))\n",
    "\n",
    "dry_d['M_bot_ECw'] = predict.WaterEC(Soil(bulk_ec_dc = dry_d.M_bot_EC.values,\n",
    "                                          water = dry_d.M_bot_W.values,\n",
    "                                          temperature = dry_d.M_bot_T.values+273.15))\n",
    "\n",
    "if profile_prefix == 'proefhoeve':\n",
    "    water_ec_10cm = dry_d.P_top_ECw.values[0]\n",
    "    water_ec_50cm = dry_d.P_bot_ECw.values[0]\n",
    "    water_ec_mean = (water_ec_10cm + water_ec_50cm)/2\n",
    "\n",
    "elif profile_prefix == 'middelkerke':\n",
    "    water_ec_10cm = dry_d.M_top_ECw.values[0]\n",
    "    water_ec_50cm = dry_d.M_bot_ECw.values[0]\n",
    "    water_ec_mean = (water_ec_10cm + water_ec_50cm)/2\n",
    "\n",
    "###################\n",
    "\n",
    "# -------------------------------------------------------------------------------------\n",
    "\n",
    "clay_50cm = np.mean(ds_c.clay[ds_c['depth']==50])\n",
    "clay_10cm = np.mean(ds_c.clay[ds_c['depth']==10])\n",
    "bd_50cm = np.mean(ds_c.bd[ds_c['depth']==50])\n",
    "bd_10cm = np.mean(ds_c.bd[ds_c['depth']==10])\n",
    "water_ec_hp_50cm = np.mean(ds_c.water_ec_hp[ds_c['depth']==50])\n",
    "water_ec_hp_10cm = np.mean(ds_c.water_ec_hp[ds_c['depth']==10])\n",
    "water_ec_hp_50cm_t = np.mean(ds_c.water_ec_hp_t[ds_c['depth']==50])\n",
    "water_ec_hp_10cm_t = np.mean(ds_c.water_ec_hp_t[ds_c['depth']==10])\n",
    "clay_mean = np.mean(ds_c.clay)\n",
    "bd_mean = np.mean(ds_c.bd)\n",
    "water_ec_hp_mean = np.mean(ds_c.water_ec_hp)\n",
    "water_ec_hp_mean_t = np.mean(ds_c.water_ec_hp_t)\n",
    "temp_50cm = np.mean(ds_c.temp[ds_c['depth']==50])\n",
    "temp_10cm = np.mean(ds_c.temp[ds_c['depth']==10])\n",
    "temp_mean = np.mean(ds_c.temp)\n",
    "vwc_50cm = np.mean(ds_c.vwc[ds_c['depth']==50])\n",
    "vwc_10cm = np.mean(ds_c.vwc[ds_c['depth']==10])\n",
    "vwc_mean = np.mean(ds_c.vwc) # 0.289 Proef\n",
    "\n",
    "f_ec = 9000\n",
    "t_conv = 273.15\n",
    "t_mean_conv = temp_mean+t_conv # 297.28 Proef\n",
    "\n",
    "# Mean of observed water values\n",
    "VWC_mean = np.mean(ds_c['vwc'].values) # 0.2891 Proef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted water based on mean input inverted EC DC TC values\n",
    "water_upper = 0.05\n",
    "water_default = 0\n",
    "water_lower = -0.05\n",
    "water_upper_p = 5\n",
    "water_default_p = 0\n",
    "water_lower_p = -5\n",
    "\n",
    "def EC_diff(EC, vwc_diff):\n",
    "\n",
    "    VWC_mean_pred = predict.Water(Soil(\n",
    "                                        bulk_ec = EC,  \n",
    "                                        frequency_ec=f_ec,\n",
    "                                        clay = clay_mean,\n",
    "                                        bulk_density = bd_mean,\n",
    "                                        water_ec = water_ec_mean,\n",
    "                                        temperature = t_mean_conv\n",
    "                                    ))[0] \n",
    "\n",
    "    return ((VWC_mean_pred - VWC_mean) - vwc_diff)**2\n",
    "\n",
    "# EC expected\n",
    "EC_5 = minimize(EC_diff, 0.01, args=(water_upper), bounds= [(0, 1)], method='Nelder-Mead')\n",
    "EC_upper = EC_5.x[0]\n",
    "\n",
    "EC_0 = minimize(EC_diff, 0.01, args=(water_default), bounds= [(0, 1)], method='Nelder-Mead')\n",
    "EC_00 = EC_0.x[0]\n",
    "\n",
    "EC_n5 = minimize(EC_diff, 0.01, args=(water_lower), bounds= [(0, 1)], method='Nelder-Mead')\n",
    "EC_lower = EC_n5.x[0]\n",
    "\n",
    "print(EC_upper, EC_lower, EC_00)\n",
    "\n",
    "# Difference percentual\n",
    "EC_upper_p = 100*(EC_upper - EC_00)/EC_00\n",
    "EC_lower_p = 100*(EC_00 - EC_lower)/EC_00\n",
    "print(EC_upper_p, EC_lower_p)\n",
    "\n",
    "sens_pedm_upper = water_upper_p/EC_upper_p\n",
    "sens_pedm_lower = water_lower_p/EC_lower_p\n",
    "print(sens_pedm_upper, sens_pedm_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_dir = 'temp_emp_04' \n",
    "infile_name = 'infile_s04.csv'\n",
    "os.makedirs(temp_dir, exist_ok=True)\n",
    "temp_file = os.path.join(temp_dir,infile_name)\n",
    "\n",
    "filename = f\"{now}_{emfile_prefix}_parameters_04.json\"\n",
    "filepath = os.path.join(temp_dir,filename)\n",
    "file = open(filepath, 'w')\n",
    "\n",
    "file.write('\\t\"EC_00\":\"{}\",'.format(EC_00) + '\\n')\n",
    "file.write('\\t\"EC_upper_p\":\"{}\",'.format(EC_upper_p) + '\\n')\n",
    "file.write('\\t\"EC_lower_p\":\"{}\",'.format(EC_lower_p) + '\\n')\n",
    "\n",
    "file.write('\\t\"sens_pedm_upper\":\"{}\",'.format(sens_pedm_upper) + '\\n')\n",
    "file.write('\\t\"sens_pedm_lower\":\"{}\",'.format(sens_pedm_lower) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inversion: configure  input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sensor settings\n",
    "config['instrument_code'] = 'Dualem-21HS' # instrument code\n",
    "config['instrument_height'] = 0.165     # instrument height\n",
    "config['instrument_orientation'] = 'HCP'    # instrument orientation\n",
    "\n",
    "# Remove coils for inversion?\n",
    "config['remove_coil'] = True    # set to True if you want to remove coils in the inversion process\n",
    "config['coil_n'] = [4,5]    # indexes of coils to remove (cf. emagpy indexing)\n",
    "                            # for Proefhoeve, coils 0 (HCP05) and 1 (PRP06) are best\n",
    "                            # removed, for Middelkerke coils 4 (HCP4.0) and 5 (PRP4.1)\n",
    "\n",
    "# Inversion parameters\n",
    "config['fs_emp'] = 'FSeq' #'CS', 'FSlin' or 'FSeq'\n",
    "config['opt_method'] = 'L-BFGS-B'  # mMinimize = ['L-BFGS-B','TNC','CG','Nelder-Mead'] --> https://docs.scipy.org/doc/scipy/reference/optimize.html \n",
    "                                # mMCMC = ['ROPE','DREAM', 'MCMC'] # ??? 'SCEUA' ??? --> https://spotpy.readthedocs.io/en/latest/ \n",
    "                                # mOther = ['ANN','Gauss-Newton','GPS'] (ANN requires tensorflow)\n",
    "config['constrain']=True\n",
    "config['regularization'] = 'l2'\n",
    "config['alpha'] = 0.07\n",
    "\n",
    "# Reference profile for starting model (conductivity values)\n",
    "config['start_avg'] = False     # take average of input resistivity profiles per layer as starting model\n",
    "                                # if false, reference profile is taken as starting model\n",
    "\n",
    "# Define the interfaces depths between layers for starting model and inversion\n",
    "#           (number of layers = len(config['interface'])+1)\n",
    "config['n_int'] = True # if True custom interfaces are defined (via config['interface']), \n",
    "                        # otherwise reference profile interfaces are used\n",
    "\n",
    "config['interface'] = [0.3, \n",
    "                       0.6, \n",
    "                       1.0,\n",
    "                       2.0\n",
    "                        ] # depths to custom model interfaces\n",
    "# Inversion constraining\n",
    "\n",
    "config['custom_bounds'] = True\n",
    "config['bounds'] = [(10, 55), (20, 120), (50, 335), (50, 250), (10, 50)]\n",
    "\n",
    "# remove profiles at transect edges\n",
    "config['n_omit'] =  10 # number of profiles to exclude from the start\n",
    "                       # and end of the ERT transect (none = 0) for the inversion\n",
    "                       # a total of 60 profiles is available, for middelkerke\n",
    "                       # 120 profiles are available \n",
    "\n",
    "if config['constrain']:\n",
    "    if config['custom_bounds']:\n",
    "        bounds = config['bounds']\n",
    "\n",
    "if config['n_int'] == False and config['custom_bounds']:\n",
    "    print('Check if bounds and number of interfaces match')\n",
    "\n",
    "# Geographic operations (if needed)\n",
    "c_transform = False\n",
    "c_utmzone = '31N'\n",
    "c_target_cs = 'EPSG:31370'\n",
    "\n",
    "# ---------------------------------------------------------------------------- #\n",
    "# ---------------------------------------------------------------------------- #\n",
    "\n",
    "# Datetime for filename\n",
    "now = (datetime.datetime.now())\n",
    "now = now.strftime(\"%y%m%d_%H%M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------- #\n",
    "# ---------------------------------------------------------------------------- #\n",
    "\n",
    "if s_site == 'P':\n",
    "    profile_prefix = 'proefhoeve'\n",
    "    if config['instrument_code'] == 'Dualem-21HS':\n",
    "        emfile_prefix = 'proefhoeve_21HS'\n",
    "    else: \n",
    "        emfile_prefix = 'proefhoeve_421S'\n",
    "else:\n",
    "    profile_prefix = 'middelkerke'\n",
    "    emfile_prefix = 'middelkerke_421S'\n",
    "    # check if correct instrument (only 421S data available for Middelkerke)\n",
    "    if config['instrument_code'] == 'Dualem-21HS':\n",
    "        config['instrument_code'] = 'Dualem-421S'\n",
    "\n",
    "inv_folder = os.path.join(datafolder, 'inverted')\n",
    "os.makedirs(inv_folder, exist_ok=True) \n",
    "cal_folder = os.path.join(datafolder, 'calibrated')\n",
    "ert_file = os.path.join(datafolder, f'{profile_prefix}-profiles.csv')\n",
    "em_rec = os.path.join(cal_folder, f'{emfile_prefix}_transect_calibrated_rECa.csv')\n",
    "em_lin = os.path.join(cal_folder,f'{emfile_prefix}_transect_calibrated_LIN.csv')\n",
    "em_survey = os.path.join(cal_folder, f'{emfile_prefix}_calibrated_rECa.csv')\n",
    "samplocs = os.path.join(datafolder, f'{profile_prefix}_samps.csv')\n",
    "\n",
    "if em_intype == 'rec':\n",
    "    infile = em_rec\n",
    "elif em_intype == 'survey':\n",
    "    infile = em_survey\n",
    "else:\n",
    "    infile = em_lin\n",
    "\n",
    "instrument = Initialize.Instrument(config['instrument_code'],\n",
    "                                    instrument_height=config['instrument_height'],\n",
    "                                    instrument_orientation=config['instrument_orientation']\n",
    "                                    )\n",
    "\n",
    "# Column names for emapgy input\n",
    "emp_21HS = [f\"HCP0.5f9000{config['instrument_height']}\", 'PRP0.6f9000h0.165', 'HCP1.0f9000h0.165', 'PRP1.1f9000h0.165',\t'HCP2.0f9000h0.165', 'PRP2.1f9000h0.165',\n",
    "            'HCP0.5f9000h0.165_inph', 'PRP0.6f9000h0.165_inph', 'HCP1.0f9000h0.165_inph',\n",
    "            'PRP1.1f9000h0.165_inph', 'HCP2.0f9000h0.165_inph', 'PRP2.1f9000h0.165_inph'\n",
    "            ]\n",
    "\n",
    "emp_421S = ['HCP1.0f9000h0.165', 'PRP1.1f9000h0.165',\t'HCP2.0f9000h0.165', 'PRP2.1f9000h0.165', 'HCP4.0f9000h0.165', 'PRP4.1f9000h0.165', \n",
    "            'HCP1.0f9000h0.165_inph', 'PRP1.1f9000h0.165_inph', 'HCP2.0f9000h0.165_inph', 'PRP2.1f9000h0.165_inph',\n",
    "            'HCP4.0f9000h0.165_inph', 'PRP4.1f9000h0.165_inph',\n",
    "            ]\n",
    "\n",
    "if config['opt_method'] == 'Gauss-Newton':\n",
    "    config['regularization'] = 'l2'\n",
    "\n",
    "# Datetime for filename\n",
    "now = (datetime.datetime.now())\n",
    "now = now.strftime(\"%y%m%d_%H%M\")\n",
    "\n",
    "# 1.0 Data import and structuring into dataframe\n",
    "ert_p = pd.read_csv(ert_file, sep=',', header=0)\n",
    "em_rec = pd.read_csv(em_rec, sep=',', header=0)\n",
    "em_lin = pd.read_csv(em_lin, sep=',', header=0)\n",
    "em_survey = pd.read_csv(em_survey, sep=',', header=0)\n",
    "samples = pd.read_csv(samplocs, sep=',', header=0)\n",
    "\n",
    "\n",
    "if c_transform:\n",
    "    # Create a new filename with the target EPSG code\n",
    "    em_rec = utm_to_epsg(em_rec, c_utmzone, target_epsg=c_target_cs)\n",
    "    em_lin = utm_to_epsg(em_lin, c_utmzone, target_epsg=c_target_cs)\n",
    "    em_survey = utm_to_epsg(em_survey, c_utmzone, target_epsg=c_target_cs)\n",
    "\n",
    "instrument = Initialize.Instrument(config['instrument_code'],\n",
    "                                    instrument_height=config['instrument_height'],\n",
    "                                        instrument_orientation=config['instrument_orientation']\n",
    "                                        )\n",
    "\n",
    "em_samples = get_coincident(em_survey,samples)\n",
    "\n",
    "# ---------------------------------------------------------------------------- #\n",
    "# Get ERT profiles\n",
    "# ---------------- #\n",
    "# Group the data by profile ID for efficient access to each profile\n",
    "profiles = ert_p.groupby('ID')\n",
    "\n",
    "# Exclude the first and last n_omit profiles\n",
    "unique_ids = ert_p['ID'].unique()\n",
    "\n",
    "if config['n_omit'] == 0:\n",
    "    ert_final = ert_p.copy()\n",
    "else:\n",
    "    if config['n_omit']*2 >= len(unique_ids):\n",
    "        warnings.warn('!!! You removed all profiles !!! Change value for config[n_omit]')\n",
    "        raise KeyboardInterrupt\n",
    "    else:\n",
    "        selected_ids = unique_ids[config['n_omit']:-config['n_omit']]\n",
    "        ert_p = ert_p.loc[ert_p['ID'].isin(selected_ids)]\n",
    "        ert_final = ert_p.copy()\n",
    "\n",
    "dataset_name = 'Resistivity(ohm.m)'  # The variable of interest\n",
    "\n",
    "# convert resistivity to conductivity and modify column names\n",
    "\n",
    "ert_final[dataset_name] = (1/ert_final[dataset_name])\n",
    "dc_corr = ert_final.copy()\n",
    "dc_corr[dataset_name] = predict.BulkEC(Soil(\n",
    "                                                frequency_ec = 9000,\n",
    "                                                bulk_ec_dc = dc_corr[dataset_name].values\n",
    "                                                ))\n",
    "\n",
    "ert_final.loc[:, dataset_name] = ert_final[dataset_name]*1000\n",
    "dc_corr.loc[:,dataset_name] = dc_corr[dataset_name]*1000\n",
    "ert_final = ert_final.rename(columns={\"Resistivity(ohm.m)\": \"EC(mS/m)\"})\n",
    "dc_corr = dc_corr.rename(columns={\"Resistivity(ohm.m)\": \"EC(mS/m)\"})\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# Columns containing the resistivity data\n",
    "data_column = ['EC(mS/m)']\n",
    "# Assuming ert_final is your DataFrame with profile data\n",
    "all_profiles_df, uniform_intervals = check_uniformity_and_interpolate(\n",
    "    dc_corr, 'ID', 'z', *data_column\n",
    ")\n",
    "\n",
    "dataset_name = 'EC(mS/m)'  # The variable of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config['reference_profile'] = 11\n",
    "\n",
    "if config['reference_profile'] not in all_profiles_df['ID'].unique():\n",
    "    warnings.warn(\"Warning: the reference profile ID does not exist. Provide correct profile ID.\")\n",
    "    raise KeyboardInterrupt\n",
    "else:\n",
    "    profile_id = config['reference_profile']\n",
    "\n",
    "# Create new layer configuration for prior model based on ERT data\n",
    "if config['n_int']:\n",
    "    new_int = config['interface']\n",
    "    merged_df = merge_layers(all_profiles_df, new_int,'EC(mS/m)')\n",
    "else:\n",
    "    merged_df = all_profiles_df\n",
    "comparedf = merged_df.copy()\n",
    "\n",
    "# Plot original and (merged and) DC corrected reference profile\n",
    "if config['n_int']:\n",
    "    plot_title = 'Original vs merged & DC corrected data'\n",
    "    first_in = .1\n",
    "else: \n",
    "    plot_title = 'Original vs DC corrected data'\n",
    "    first_in = .0\n",
    "ert_eval = ert_final.copy()\n",
    "ert_eval['z'] = ert_eval['z'].values + first_in\n",
    "\n",
    "plot_profile(ert_eval, profile_id, dataset_name, compare=True, compare_df = comparedf, compare_name = 'EC(mS/m)', block=True, plot_title=plot_title)\n",
    "\n",
    "# Get prior model info\n",
    "def generate_forward_model_inputs(df, profile_id_col, depth_col, res_col):\n",
    "    models = {}  # Dictionary to store models by profile ID\n",
    "\n",
    "    for profile_id, group in df.groupby(profile_id_col):\n",
    "        # Assuming uniform interval after previous interpolation\n",
    "        uniform_interval = abs(group[depth_col].diff().iloc[1])\n",
    "        #print(uniform_interval)\n",
    "        num_layers = len(group[res_col])\n",
    "                # Thicknesses are the intervals between depths, except for the last value which does not define a new layer\n",
    "        thick = np.full(num_layers - 1, uniform_interval)\n",
    "        thick[0] = 2 * thick[0]\n",
    "        # Conductivity is the inverse of resistivity\n",
    "        con = group[res_col].values/1000\n",
    "        # Permittivity is the epsilon_0 for all layers\n",
    "        perm = np.full(num_layers, constants.epsilon_0)\n",
    "        sus = np.zeros(num_layers)\n",
    "        # Create model instance\n",
    "        M = Initialize.Model(thick, sus[::-1], con[::-1], perm[::-1])\n",
    "        \n",
    "        # Store the model instance in the dictionary with the profile ID as the key\n",
    "        models[profile_id] = M\n",
    "    return models\n",
    "\n",
    "models = generate_forward_model_inputs(merged_df, 'ID', 'z', 'EC(mS/m)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# -------------------------------------------------------------------- #\n",
    "\n",
    "# \n",
    "profile_data = merged_df[merged_df['ID'] == profile_id].copy()\n",
    "res_col = 'EC(mS/m)'\n",
    "depth = 'z'\n",
    "max_ert_depth = ert_final['z'].abs().max()\n",
    "\n",
    "# \n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# A. Test run on the reference profile (config['reference_profile'])\n",
    "#       and plot the results\n",
    "\n",
    "if not config['n_int']:\n",
    "    first_lay = profile_data[depth].iloc[-1].round(decimals=1)\n",
    "    second_lay = profile_data[depth].iloc[-2].round(decimals=1)\n",
    "    if first_lay == 0:\n",
    "        profile_data[depth]=profile_data[depth] +second_lay\n",
    "    else:\n",
    "        profile_data[depth]=profile_data[depth] +first_lay\n",
    "    thick = -profile_data[depth].iloc[1:].values\n",
    "    #thick = -profile_data[depth].values\n",
    "else:\n",
    "    thick = -profile_data[depth].values\n",
    "\n",
    "con = profile_data[res_col].values/1000\n",
    "ref_len = len(con)\n",
    "num_layers = len(con)\n",
    "perm = np.full(num_layers, constants.epsilon_0)\n",
    "sus = np.zeros(num_layers)\n",
    "\n",
    "# # Create model instance\n",
    "M = Initialize.Model(thick, sus[::-1], con[::-1], perm[::-1])\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "dataset_name = 'EC(mS/m)'\n",
    "layers_interfaces = np.cumsum(models[profile_id].thick)\n",
    "layers_interfaces = np.insert(layers_interfaces, 0, 0)\n",
    "profile_data = ert_final[ert_final['ID'] == profile_id]\n",
    "\n",
    "fig, axr = plt.subplots(figsize=(5, 10))\n",
    "axr.set_xlabel('EC [mS/m]')\n",
    "axr.set_ylabel('depth [m]')\n",
    "axr.plot((profile_data[dataset_name]),profile_data['z'], label='original (DC) ERT EC',)\n",
    "if not config['n_int']: \n",
    "    axr.plot(con[:-1]*1000,-thick, '.', label='Model EC 9khz',color = 'red')\n",
    "else:\n",
    "    axr.plot(con*1000,-thick, '.', label='Model EC 9khz',color = 'red')\n",
    "axr.set_title(f'Reference profile: ID {profile_id}')\n",
    "\n",
    "conductivities = con*1000\n",
    "print('conductivities', conductivities)\n",
    "\n",
    "ec_cols_ref = []\n",
    "if 'end' in config['interface']:\n",
    "    config['interface'].remove('end')\n",
    "# Get conductivity stats for bounds\n",
    "if config['n_int']:\n",
    "    if 'end' in ec_cols_ref:\n",
    "        ec_cols_ref.remove('end')\n",
    "    ec_cols_ref = config['interface']\n",
    "    ec_cols_ref.append('end')\n",
    "    mod_layers = thick[1:]\n",
    "else:\n",
    "    if len(conductivities) == len(thick):\n",
    "        mod_layers = thick[1:]\n",
    "        print(f\"length modlayers = {len(mod_layers)} with {len(conductivities)} conductivities\")\n",
    "    elif len(conductivities) == (len(thick)+1):\n",
    "        mod_layers = thick\n",
    "        print(f\"length modlayers = {len(mod_layers)} with {len(conductivities)} conductivities\")\n",
    "    else:\n",
    "        raise ValueError(f\"Check length of conductivities ({len(conductivities)}) and layers ({len(thick)}) arrays!!\")\n",
    "    \n",
    "    ec_cols_ref = np.round(layers_interfaces,decimals=1).tolist()\n",
    "ec_df = pd.DataFrame(columns=ec_cols_ref)\n",
    "\n",
    "# \n",
    "for i in merged_df['ID'].unique(): \n",
    "    profile_data = merged_df[merged_df['ID'] == i].copy()\n",
    "    if not config['n_int']:\n",
    "        if abs(profile_data.iloc[0]['z']) > max((list(map(abs, ec_cols_ref)))):\n",
    "            #print(f'removed {profile_data.iloc[0][\"z\"]}')\n",
    "            profile_data = profile_data.drop(profile_data.iloc[0].name)\n",
    "        elif abs(profile_data.iloc[-1]['z']) < 0.1:\n",
    "            #print(f'removed {profile_data.iloc[-1][\"z\"]}')\n",
    "            profile_data = profile_data.drop(profile_data.iloc[-1].name)\n",
    "    res_col = 'EC(mS/m)'\n",
    "    depth = 'z' \n",
    "    con_m = profile_data[res_col].values\n",
    "    layers_interfaces = np.cumsum(models[i].thick)\n",
    "    layers_interfaces = np.insert(layers_interfaces, 0, 0)\n",
    "    num_layers = len(con)\n",
    "    perm = np.full(num_layers, constants.epsilon_0)\n",
    "    sus = np.zeros(num_layers)\n",
    "\n",
    "    first_lay = profile_data[depth].iloc[-1].round(decimals=1)\n",
    "    second_lay = profile_data[depth].iloc[-2].round(decimals=1)\n",
    "\n",
    "    if not config['n_int']:\n",
    "        first_lay = profile_data[depth].iloc[-1].round(decimals=1)\n",
    "        second_lay = profile_data[depth].iloc[-2].round(decimals=1)\n",
    "        if first_lay == 0:\n",
    "            profile_data[depth]=profile_data[depth] +second_lay\n",
    "        else:\n",
    "            profile_data[depth]=profile_data[depth] +first_lay\n",
    "        thick = -profile_data[depth].iloc[1:].values\n",
    "    else:\n",
    "        thick = -profile_data[depth].values\n",
    "\n",
    "    ec_df = pd.concat([ec_df, pd.DataFrame([np.flip(con_m)], columns=ec_cols_ref)])\n",
    "\n",
    "ec_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "ec_stats = ec_df.describe().loc[['min', 'max', 'std', '50%', 'mean']]\n",
    "ec_stats.rename(index={'50%': 'median'}, inplace=True)\n",
    "ec_stats.loc['min_sd'] = ec_stats.loc['min'] - 2 * ec_stats.loc['std']\n",
    "ec_stats.loc['max_sd'] = ec_stats.loc['max'] + 2 * ec_stats.loc['std']\n",
    "\n",
    "position = -thick\n",
    "\n",
    "\n",
    "# define parameters for inversion starting model\n",
    "# --------------------------------------------- #\n",
    "\n",
    "if not config['n_int']:\n",
    "    minstat = np.flipud(ec_stats.loc['min'].values[1:])\n",
    "    maxstat = np.flipud(ec_stats.loc['max'].values[1:])\n",
    "    start_mod = ec_stats.loc['mean'].values[1:]\n",
    "    boundcols = ec_cols_ref[:-1]\n",
    "else:\n",
    "    minstat = np.flipud(ec_stats.loc['min'].values)\n",
    "    maxstat = np.flipud(ec_stats.loc['max'].values)\n",
    "    start_mod = ec_stats.loc['mean'].values\n",
    "\n",
    "axr.plot(np.flipud(start_mod),position, \n",
    "            '*', \n",
    "            label='average conductivity',\n",
    "            color = 'green',\n",
    "            alpha = 0.5)\n",
    "axr.plot(minstat,position, \n",
    "            '.', \n",
    "            label='min',\n",
    "            color = 'black',\n",
    "            alpha = 0.2)\n",
    "axr.plot(maxstat,position, \n",
    "            '+', \n",
    "            label='max',\n",
    "            color = 'black',\n",
    "            alpha = 0.25)\n",
    "\n",
    "axr.legend()\n",
    "if config['constrain']:\n",
    "    if config['custom_bounds']:\n",
    "        bounds = config['bounds']\n",
    "    else:\n",
    "        bounds = []\n",
    "        for i, name in enumerate(ec_cols_ref):\n",
    "            if ec_stats.loc['min_sd'][name] > 0:\n",
    "                min = ec_stats.loc['min_sd'][name]\n",
    "            elif ec_stats.loc['min'][name] > 0:\n",
    "                min = ec_stats.loc['min'][name]\n",
    "            else:\n",
    "                min = 10\n",
    "            max = ec_stats.loc['max_sd'][name]\n",
    "            min_max = tuple([min,max])\n",
    "            bounds.append(min_max)\n",
    "        bounds = np.round(bounds, decimals=0)\n",
    "        if not config['n_int'] and not config['custom_bounds']:\n",
    "            bounds = bounds[1:]\n",
    "        print(f'autobounds = {bounds}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform inversion on sampling locations (to be used in pedophysical modelling)\n",
    "\n",
    "if 'code' in em_samples.columns:\n",
    "    em_samples = em_samples.rename(columns={'code': 'ID'})\n",
    "\n",
    "i = instrument.niter\n",
    "n = 4\n",
    "em_samples.columns.values[n:n+i]\n",
    "\n",
    "if config['instrument_code'] == 'Dualem-21HS':\n",
    "    new_columns = emp_21HS\n",
    "else:\n",
    "    new_columns = emp_421S\n",
    "\n",
    "if len(new_columns) != i:\n",
    "    raise ValueError(\"The length of new_columns must be equal to the number of columns to rename\")\n",
    "else:\n",
    "    em_samples.columns.values[n:n+i] = new_columns\n",
    "\n",
    "em_samples.to_csv(temp_file)\n",
    "\n",
    "# transect inversion settings\n",
    "\n",
    "s_rec = Problem()\n",
    "s_rec.createSurvey(temp_file)\n",
    "#t_rec.rollingMean(window=12)\n",
    "\n",
    "s_rec.setInit(\n",
    "    depths0=np.flipud(mod_layers),\n",
    "    conds0=conductivities\n",
    "    )\n",
    "\n",
    "if config['remove_coil']:\n",
    "    if type(config['coil_n']) == list:\n",
    "        config['coil_n'] = sorted(config['coil_n'])\n",
    "        for i in enumerate(config['coil_n']):\n",
    "            r_coil = s_rec.coils[(config['coil_n'][i[0]]-i[0])]\n",
    "            # print(f'removing {r_coil}')\n",
    "            s_rec.removeCoil(config['coil_n'][i[0]]-i[0])\n",
    "    else:\n",
    "        s_rec.removeCoil(config['coil_n'])\n",
    "\n",
    "print(f'Data used for inversion: {s_rec.coils}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invert using ROPE solver (RObust Parameter Estimation)\n",
    "warnings.filterwarnings('ignore')\n",
    "opt_meth = config['opt_method']\n",
    "inv_meth = config['fs_emp']\n",
    "reg_meth = config['regularization']\n",
    "alph_param = config['alpha']\n",
    "if opt_meth in ['MCMC', 'ROPE']:\n",
    "    if config['constrain']:\n",
    "        \n",
    "        print(f'Constrained inversion using {inv_meth} with {opt_meth}, reg={reg_meth}, alpha={alph_param}')\n",
    "        s_rec.invert(forwardModel=config['fs_emp'], method=opt_meth, \n",
    "                regularization=reg_meth, alpha=alph_param, \n",
    "                bnds=bounds\n",
    "                )\n",
    "\n",
    "    else:\n",
    "        print(f'Inversion using {inv_meth} with {opt_meth}, reg={reg_meth}, alpha={alph_param}')\n",
    "        s_rec.invert(forwardModel=config['fs_emp'], method=opt_meth, \n",
    "        regularization=reg_meth, alpha=alph_param, njobs=-1\n",
    "        )\n",
    "\n",
    "else:\n",
    "    print(f'Inversion using {inv_meth} with {opt_meth}, reg={reg_meth}, alpha={alph_param}')\n",
    "    s_rec.invert(forwardModel='FSeq', method='Gauss-Newton', alpha=alph_param,regularization=reg_meth)\n",
    "s_rec.showOne2one()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1: Plot the inversion results and put outcomes into a pandas dataframe\n",
    "# ------------------------------------------------------------------------\n",
    "csv_filename = f'{now}_{emfile_prefix}_inverted_samples_{opt_meth}_04.csv'\n",
    "\n",
    "# ******************************************************************** #\n",
    "\n",
    "# Plot inversion outcomes down to a max depth of 2 m, and plotting the data\n",
    "# based on their true coordinates along the transect (dist=True).\n",
    "s_rec.showResults(dist=True, errorbar = True) \n",
    "\n",
    "# Extracting the values from the first row of the transect.depths[0] array\n",
    "depth_values = s_rec.depths[0][0]\n",
    "\n",
    "# Creating the custom column names for layer_cols\n",
    "layer_cols = ['EC_{:.2f}'.format(d) for d in depth_values] + ['EC_end']\n",
    "\n",
    "# Combining the data from the 'x', 'y' columns and the transect.models[0] array\n",
    "data = np.c_[s_rec.surveys[0].df[['x', 'y']].values, s_rec.models[0]]\n",
    "\n",
    "# Creating the final dataframe with the desired column names\n",
    "ds_inv = pd.DataFrame(data, columns=['x', 'y'] + layer_cols)\n",
    "ds_inv['pos'] = em_samples['ID'].to_numpy()\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "# Export the dataframe as a csv-file\n",
    "outfile_transect = os.path.join(inv_folder, csv_filename)\n",
    "ds_inv.to_csv(outfile_transect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_columns = ds_inv.columns[3:-1]\n",
    "ds_c[inv_columns] = np.nan\n",
    "\n",
    "for idc, c in enumerate(inv_columns):\n",
    "\n",
    "    for i in range(len(ds_inv.x)):\n",
    "        ds_c.loc[ds_c.code == i+1, c] = ds_inv.loc[i, c]\n",
    "\n",
    "def closest_ec(row):\n",
    "    depth = row['depth']\n",
    "    # Filter columns that start with 'EC_' but not 'EC_end'\n",
    "    ec_cols = [col for col in row.index if col.startswith('EC_') and col != 'EC_end']\n",
    "    # Convert the part after 'EC_' to float and calculate the absolute difference with depth\n",
    "    differences = {col: abs(depth/100 - float(col.split('_')[1])) for col in ec_cols}\n",
    "    # Find the column name with the minimum difference\n",
    "    closest_col = min(differences, key=differences.get)\n",
    "    return row[closest_col]\n",
    "\n",
    "# Apply the function to each row\n",
    "ds_c['bulk_ec_inv'] = ds_c.apply(closest_ec, axis=1)\n",
    "\n",
    "#Obtain EC DC TC\n",
    "ds_c['bulk_ec_dc_tc_inv'] = predict.BulkECDCTC(Soil(temperature = ds_c.temp.values+273.15,\n",
    "                                                    frequency_ec = 9e3,\n",
    "                                                    bulk_ec = ds_c.bulk_ec_inv.values/1000))\n",
    "# Mean of input inverted EC DC TC values\n",
    "EC_mean = np.mean(ds_c['bulk_ec_dc_tc_inv'].values) \n",
    "print('EC_mean', EC_mean)\n",
    "\n",
    "#### Uncertainty inversion parameters\n",
    "EC = 0.06980570031133289\n",
    "\n",
    "### ROPE uncertainty\n",
    "inv_results = [0.075, 0.069, 0.073, 0.0687, 0.0695, 0.0719, 0.0745, 0.0709, 0.0644, 0.0708]\n",
    "\n",
    "ROPE_inv_upper_p = 100*(-EC + np.max(inv_results))/(np.max(inv_results))\n",
    "ROPE_inv_lower_p = 100*(-EC + np.min(inv_results))/(np.min(inv_results))\n",
    "print('ROPE_inv_upper_p, ROPE_inv_lower_p', ROPE_inv_upper_p, ROPE_inv_lower_p)\n",
    "\n",
    "### Alpha uncertainty\n",
    "Alpha_upper = np.inf\n",
    "Alpha_lower = 0.001\n",
    "Alpha = 0.07\n",
    "\n",
    "Alpha_upper_p = 100*(Alpha_upper - Alpha)/Alpha\n",
    "Alpha_lower_p = 100*(-Alpha + Alpha_lower)/Alpha\n",
    "print('Alpha_upper_p, Alpha_lower_p', Alpha_upper_p, Alpha_lower_p)\n",
    "\n",
    "sens_alpha_upper = EC_upper_p/Alpha_upper_p\n",
    "sens_alpha_lower = EC_lower_p/Alpha_lower_p\n",
    "print('sens_alpha_upper, sens_alpha_lower', sens_alpha_upper, sens_alpha_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.write('\\t\"EC_mean\":\"{}\",'.format(EC_mean) + '\\n')\n",
    "file.write('\\t\"ROPE_inv_upper_p\":\"{}\",'.format(ROPE_inv_upper_p) + '\\n')\n",
    "file.write('\\t\"ROPE_inv_lower_p\":\"{}\",'.format(ROPE_inv_lower_p) + '\\n')\n",
    "\n",
    "file.write('\\t\"input file + path\": \"{}\",'.format(infile) + '\\n\\n')\n",
    "file.write('\\t\"instrument\": \"{}\",'.format(config['instrument_code'] ) + '\\n')\n",
    "file.write('\\t\"instrument mode\": \"{}\",'.format(config['instrument_orientation']) + '\\n')\n",
    "file.write('\\t\"instrument height (m)\": {:.3f},'.format(config['instrument_height']) + '\\n')\n",
    "\n",
    "if config['remove_coil']:\n",
    "    rem_coils = instrument.cc_names[config['coil_n']]\n",
    "    file.write('\\t\"configurations not used in inversion\": \"{}\",'.format(rem_coils) + '\\n\\n')\n",
    "\n",
    "file.write('\\t\"forward model\": \"{}\",'.format(config['fs_emp']) + '\\n')\n",
    "file.write('\\t\"optimisation method\":\"{}\",'.format(config['opt_method']) + '\\n')\n",
    "file.write('\\t\"regularisation\": \"{}\",'.format(config['regularization']) + '\\n')\n",
    "file.write('\\t\"alpha parameter\": \"{}\",'.format(alph_param) + '\\n\\n')\n",
    "file.write('\\t\"reference EC profile\":\"{}\",'.format(config['reference_profile']) + '\\n')\n",
    "\n",
    "if config['constrain']:\n",
    "    file.write('\\t \"constrained inversion\":' + '\\n')\n",
    "    if config['n_int']:\n",
    "        file.write('\\t\"custom interface boundaries\": \"{}\"\\n'.format(config['interface']) + '\\n')\n",
    "    if config['custom_bounds']:\n",
    "        file.write('\\t\"custom inversion constraints (bnds)\": \"{}\" \\n'.format(config['bounds']) + '\\n')\n",
    "    else:\n",
    "        file.write('\\t\"automated inversion constraints (bnds)\": \"{}\"\\n'.format(bounds) + '\\n')\n",
    "\n",
    "file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
